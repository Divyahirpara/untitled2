,,
ID,title,paper_text
1,Self-Organization of Associative Database and Its Applications,"INTRODUCTION Let a mapping f : X -+ Y be given. Here, X is a finite or infinite set, and Y is another finite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly from X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some estimate j : X -+ Y of f to make small, the estimation error in some measure. Usually we say that: the faster the decrease of estimation error with increase of the number of samples, the better the learning machine. However, such expression on performance is incomplete. Since, it lacks consideration on the candidates of J of j assumed preliminarily. Then, how should we find out good learning machines? To clarify this conception, let us discuss for a while on some types of learning machines. And, let us advance the understanding of the self-organization of associative database . Recently, even hardware units computing coefficients in parallel for speed-up are sold, e.g., ANZA, Mark III, Odyssey and E-1. Hence, contrarily to the parameter type, the recursive type guarantees surely that j approaches to 1 as the number of samples increases. The recursive type, if observes a sample (x"" yd, rewrites values 1,-l(X),S to I,(x)'s for some x's correlated to the sample. Hence, this type has an architecture composed of a rule for rewriting and a free memory space. Such architecture forms naturally a kind of database that builds up management systems of data in a self-organizing way. However, this database differs from ordinary ones in the following sense. It does not only record the samples already observed, but computes some estimation of l(x) for any x E X. We call such database an associative database. The first subject in constructing associative databases is how we establish the rule for rewri ting. Furthermore, let t = 1. Step 2: Increase t by 1, and put x, in. After reset a pointer n to the root, repeat the following until n arrives at some terminal node, i.e., leaf. Notations nand d(xt, x[n)), let n n mean the descendant nodes of n. =n. Otherwise, let n =n. If d(x"" r[n)) ~ Step 3: Display yIn] as the related information. Next, put y, in. If yIn] = y"" back to step 2. Otherwise, first establish new descendant nodes n and n.Here, the loop of step 2-3 can be stopped at any time and also can be continued. Now, suppose that gate elements, namely, artificial ""synapses"" that play the role of branching by d are prepared. Then, we obtain a new style of neural network with gate elements being randomly connected by this algorithm. LETTER RECOGNITION Recen tly, the vertical slitting method for recognizing typographic English letters3 , the elastic matching method for recognizing hand written discrete English letters4 , the global training and fuzzy logic search method for recognizing Chinese characters written in square styleS, etc. are published. The self-organization of associative database realizes the recognition of handwritten continuous English letters. The letter recognizer uses a parallelogram window that at least can cover the maximal letter (Fig. 2), and processes the sequence of letters while shifting the window. That is, the recognizer scans a word in a slant direction. And, it places the window so that its left vicinity may be on the first black point detected. Then, the window catches a letter and some part of the succeeding letter. If recognition of the head letter is performed, its end position, namely, the boundary line between two letters becomes known. Hence, by starting the scanning from this boundary and repeating the above operations, the recognizer accomplishes recursively the task. Thus the major problem comes to identifying the head letter in the window. Considering it, we define the following. ? Regard window images as x's, and define X accordingly. ? For a (x, x) E X x X, denote by B a black point in the left area from the boundary on window image X. Project each B onto window image x. Then, measure the Euclidean distance 6 between fj and a black point B on x being the closest to B. Let d(x, x) be the summation of 6's for all black points B's on x divided by the number of B's. ? Regard couples of the ""reading"" and the position of boundary as y's, and define Y accordingly. An operator teaches the recognizer in interaction the relation between window image and reading& boundary with algorithm 3. Precisely, if the recalled reading is incorrect, the operator teaches a correct reading via the console. Moreover, if the boundary position is incorrect, he teaches a correct position via the mouse. Fig. 1 shows partially a document image used in this experiment. This task is automatic excepting the periodic reset of counter, namely, it is a kind of learning without teacher. We define the identification rate by the relative frequency of correct recalls of position data in the past 100 trials. In a typical example, it converged to about 83% around time 400. At time 400, the number of levels was 202, and the levels oftree were distributed in 522. Since the identification failures of 17% can be rejected by considering the trajectory, no pro blem arises in practical use. In order to improve the identification rate, the compression ratio of camera images must be loosened. Such possibility depends on improvement of the hardware in the future. shows an example of actual motion of the robot based on the database for obstacle avoiding movement and that for position identification."
100,Storing Covariance by the Associative Long-Term Potentiation and Depression of Synaptic Strengths in the Hippocampus,"683 A MEAN FIELD THEORY OF LAYER IV OF VISUAL CORTEX AND ITS APPLICATION TO ARTIFICIAL NEURAL NETWORKS* Christopher L. Scofield Center for Neural Science and Physics Department Brown University Providence, Rhode Island 02912 and Nestor, Inc., 1 Richmond Square, Providence, Rhode Island, 02906. ABSTRACT A single cell theory for the development of selectivity and ocular dominance in visual cortex has been presented previously by Bienenstock, Cooper and Munrol. This has been extended to a network applicable to layer IV of visual cortex 2 . In this paper we present a mean field approximation that captures in a fairly transparent manner the qualitative, and many of the quantitative, results of the network theory. Finally, we consider the application of this theory to artificial neural networks and show that a significant reduction in architectural complexity is possible. A SINGLE LAYER NETWORK AND THE MEAN FIELD APPROXIMATION We consider a single layer network of ideal neurons which receive signals from outside of the layer and from cells within the layer. This assumption does not imply that each cell in the network is selective to the same pattern, (and thus that mi = mj). Rather, the assumption is that the vector sum is a constant This amounts to assuming that each cell in the network is surrounded by a population of cells which represent, on average, all possible pattern preferences. Thus the vector sum of the afferent synaptic states describing these pattern preferences is a constant independent of location. Here, in a manner similar to that in the theory of magnetism, we have replaced the effect of individual cortical cells by their average effect (as though all other cortical cells can be replaced by an 'effective' cell, figure 2). Note that we have retained all orders of synaptic traversal of the signal d. Thus, we now focus on the activity of the layer after 'relaxation' to equilibrium. In the mean field approximation we can therefore write (5) where the mean field a with =am 686 and we asume that inhibitory). Afferent Signals d Lo < 0 (the network is, on average, > Figure 2: The single layer mean field network. Detailed connectivity between all cells of the network is replaced with a single (nonmodifiable) synapse from an 'effective' cell. LEARNING IN THE CORTICAL NETWORK We will first consider evolution of the network according to a synaptic modification rule that has been studied in detail, for single cells, elsewhere!? 3. We consider the LGN - cortical synapses to be the site of plasticity and assume for maximum simplicity that there is no modification of cortico-cortical synapses. Then (6) . Lij = O. In what follows c denotes the spatial average over cortical cells, while Cj denotes the time averaged activity of the i th cortical cell. The function cj> has been discussed extensively elsewhere. Here we note that cj> describes a function of the cell response that has both hebbian and anti-hebbian regions. 687 This leads to a very complex set of non-linear stochastic equations that have been analyzed partially elsewhere 2 . In general, the afferent synaptic state has fixed points that are stable and selective and unstable fixed points that are nonselective!, 2. These arguments may now be generalized for the network. In the mean field approximation (7) The mean field, a has a time dependent component m. This varies as the average over all of the network modifiable synapses and, in most environmental situations, should change slowly compared to the change of the modifiable synapses to a single cell. Then in this approximation we can write ? (mi(a)-a) = cj>[mi(a) - a] d. (8) We see that there is a mapping mi' <-> mica) - a (9) such that for every mj(a) there exists a corresponding (mapped) point mj' which satisfies the original equation for the mean field zero theory. It can be shown 2, 4 that for every fixed point of mj( a = 0), there exists a corresponding fixed point mj( a) with the same selectivity and stability properties. The fixed points are available to the neurons if there is sufficient inhibition in the network (ILo I is sufficiently large). APPLICATION OF THE MEAN FIELD NETWORK TO LAYER IV OF VISUAL CORTEX Neurons in the primary visual cortex of normal adult cats are sharply tuned for the orientation of an elongated slit of light and most are activated by stimulation of either eye. Both of these properties--orientation selectivity and binocularity--depend on the type of visual environment experienced during a critical 688 period of early postnatal development. For example, deprivation of patterned input during this critical period leads to loss of orientation selectivity while monocular deprivation (MD) results in a dramatic shift in the ocular dominance of cortical neurons such that most will be responsive exclusively to the open eye. The ocular dominance shift after MD is the best known and most intensively studied type of visual cortical plasticity. Further, the Hopfield model allows a storage capacity which is limited to m < N memories 8, 9. This is a result of the proliferation of unwanted local minima in the 'energy' surface. Recently, Bachmann et al. l 0, have proposed a model for the relaxation of network activity in which memories of activity patterns are the sites of negative 'charges', and the activity caused by a test pattern is a positive test 'charge'. L is a parameter related to the network size. Importantly, this measure is the same for all cells; it is as if a single virtual cell was computing the distance in activity space between the current state and stored states. The result of the computation is This is a then broadcast to all of the cells in the network. generalization of the idea that the detailed activity of each cell in the network need not be fed back to each cell. Rather some global measure, performed by a single 'effective' cell is all that is sufficient in the feedback. DISCUSSION We have been discussing a formalism for the analysis of networks of ideal neurons based on a mean field approximation of the detailed activity of the cells in the network. We find that a simple assumption concerning the spatial distribution of the pattern preferences of the cells allows a great simplification of the analysis. In particular, the detailed activity of the cells of the network may be replaced with a mean field that in effect is computed by a single 'effective' cell."
1000,Bayesian Query Construction for Neural Network Models,"INTRODUCTION In this paper we consider the situation where data collection is costly, as when for example, real measurements or technical experiments have to be performed. In this situation the approach of query learning ('active data selection', 'sequential experimental design', etc.) has a potential benefit. Depending on the previously seen examples, a new input value ('query') is selected in a systematic way and the corresponding output is obtained. The motivation for query learning is that random examples often contain redundant information, and the concentration on non-redundant examples must necessarily improve generalization performance. We use a Bayesian decision-theoretic framework to derive a criterion for query construction. The criterion reflects the intended use of the predictions by an appropriate 444 Gerhard Paass. Jorg Kindermann loss function. We limit our analysis to the selection of the next data point, given a set of data already sampled. The proposed procedure derives the expected loss for candidate inputs and selects a query with minimal expected loss. There are several published surveys of query construction methods [Ford et al. 89, Plutowski White 93, Sollich 94]. Most current approaches, e.g. [Cohn 94], rely on the information matrix of parameters. Then however, all parameters receive equal attention regardless of their influence on the intended use of the model [Pronzato Walter 92]. In addition, the estimates are valid only asymptotically. Bayesian approaches have been advocated by [Berger 80], and applied to neural networks [MacKay 92]. In [Sollich Saad 95] their relation to maximum information gain is discussed. In this paper we show that by using Markov Chain Monte Carlo methods it is possible to determine all quantities necessary for the selection of a query. This approach is valid in small sample situations, and the procedure's precision can be increased with additional computational effort. With the square loss function, the criterion is reduced to a variant of the familiar integrated mean square error [Plutowski White 93]. In the next section we develop the query selection criterion from a decision-theoretic point of view. In the third section we show how the criterion can be calculated using Markov Chain Monte Carlo methods and we discuss a strategy for model selection. In the last section, the results of two experiments with MLPs are described. 2 A DECISION-THEORETIC FRAMEWORK Assume we have an input vector x and a scalar output y distributed as y """" p(y I x, w) where w is a vector of parameters. The conditional expected value is a deterministic function !(x, w) := E(y I x, w) where y = !(x, w)+? and ? is a zero mean error term. Suppose we have iteratively collected observations D(n) := ((Xl, iii), .. . , (Xn, Yn)). We get the Bayesian posterior p(w I D(n)) = p(D(n) Iw) p(w)/ J p(D(n) Iw) p(w) dw and the predictive distribution p(y I x, D(n)) = p(y I x, w)p(w I D(n)) dw if p(w) is the prior distribution. J We consider the situation where, based on some data x, we have to perform an action a whose result depends on the unknown output y. Some decisions may have more severe effects than others. The loss function L(y, a) E [0,00) measures the loss if y is the true value and we have taken the action a E A. In this paper we consider real-valued actions.Together with its still unknown y-value, x defines a new observation (x, y) and new data Den) U (x, y). To determine this risk for some given x we have to perform the following conceptual steps for a candidate query x: 1. Averaging: Integrate this quantity over the future trial inputs x distributed as p(x) and the different possible future outputs y, yielding 1';:= Ir;,x(x)p(x)p(ylx,Den)dxdy. This procedure is repeated until an x with minimal average risk is found. Since local optima are typical, a global optimization method is required. Subsequently we then try to determine whether the current model is still adequate or whether we have to increase its complexity (e.g. by adding more hidden units). Considering the substantial computing effort this favors the current loss criterion. The dots indicate the RMSE for randomly generated data (averaged over 8 experiments) using the same Bayesian prediction procedure. Because only few data points were located in the critical region of high variation the RMSE is much larger.We selected the query point with maximal current loss and determined the final query point with a hillclimbing algorithm. In this way we were rather sure to get close to the true global optimum. The main result of the experiment is summarized in the right part of MLP target function and root mean square error. shows - averaged over 3 experiments - the root mean square error between the true mean value and the posterior mean E(y I x) on the grid of 1681 inputs in relation to the sample size. Three phases of the exploration can be distinguished (see figure 3). In the beginning a search is performed with many queries on the border of the input area. After about 20 observations the algorithm knows enough detail about the true function to concentrate on the relevant parts of the input space. This leads to a marked reduction ofthe mean square error. After 40 observations the systematic part of the true function has been captured nearly perfectly. In the last phase of the experiment the algorithm merely reduces the uncertainty caused by the random noise. In contrast , the data generated randomly does not have sufficient information on the details of f(x , w), and therefore the error only gradually decreases. Because of space constraints we cannot report experiments with radial basis functions which led to similar results."
1001,"Neural Network Ensembles, Cross Validation, and Active Learning","INTRODUCTION It is well known that a combination of many different predictors can improve predictions. In the neural networks community ""ensembles"" of neural networks has been investigated by several authors, see for instance [1, 2, 3]. Most often the networks in the ensemble are trained individually and then their predictions are combined. This combination is usually done by majority (in classification) or by simple averaging (in regression), but one can also use a weighted combination of the networks. Author to whom correspondence should be addressed.This suggests the following strategy. Chose a number K :::; p. For each network in the ensemble hold out K examples for testing, where the N test sets should have minimal overlap, i. e., the N training sets should be as different as possible. If, for instance, K :::; piN it is possible to choose the K test sets with no overlap. Five identical feed-forward networks with one hidden layer of 20 units were trained independently by back-propagation using 200 random examples. For each network a cross-validation set of K examples was held out for testing as described above. Note, however, that a member of the ensemble can have such a poor generalization or be so correlated with the rest of the ensemble that it is optimal to set its weight to zero. The weights can be ""learned"" from unlabeled examples, e.g. by gradient descent minimization of the estimate of the generalization error (10). The dashed curve shows the generalization error when the weights are optimized as described above using the estimates of Ea from the cross-validation (on K exampies). Which tells us that the ambiguity is a lower bound for the weighted average of the squared error. An input pattern that yields a large ambiguity will always have a large average error. On the other hand, a low ambiguity does not necessarily imply a low error. If the individual networks are trained to a low training error on the same set of examples then both the error and the ambiguity are low on the training points. This ensures that a pattern yielding a large ambiguity cannot be in the close neighborhood of a training example. The ambiguity will to some extent follow the fluctuations in the error. Since the ambiguity is calculated from unlabeled examples the input-space can be scanned for these areas to any detail. Jesper Vedelsby This was done by training the individuals on different training sets by holding out some examples for each individual during training. This had the added advantage that these examples could be used for testing, and thereby one could obtain good estimates of the generalization error. It was discussed how to find the optimal weights for the individuals of the ensemble. For our simple test problem the weights found improved the performance of the ensemble significantly. Finally a method for active learning was described, which was based on the method of query by committee developed for classification problems. The idea is that if the ensemble disagrees strongly on an input, it would be good to find the label for that input and include it in the training set for the ensemble. It was shown how active learning improves the learning curve a lot for a simple test problem. Acknowledgements We would like to thank Peter Salamon for numerous discussions and for his implementation of linear programming for optimization of the weights. We also thank Lars Kai Hansen for many discussions and great insights, and David Wolpert for valuable comments."
1002,Using a neural net to instantiate a deformable mod,"The ink generators and background process define a mixture model. Using the assumption that each data point is generated independently given the instantiated model, P(Ilz*, M) factors into the product of the probability density of each black pixel under the mixture model. 1.2 RECOGNIZING ISOLATED DIGITS For each model, the aim of the search is to find the instantiation parameters that minimize E tot . The search starts with zero deformations and an initial guess for the affine parameters which scales the model so as to lie over the data with zero skew and rotation. A small number of generators with the same large variance are placed along the spline, forming a broad, smooth ridge of high ink-probability along the spline. 2 PREDICTING THE INSTANTIATION PARAMETERS The search procedure described above is very time consuming. However, given many examples of images and the corresponding instantiation parameters obtained by the slow method, it is possible to train a neural network to predict the instantiation parameters of novel images. These predictions provide better starting points, so the search time can be reduced. 3The schedule starts with 8 beads increasing to 60 beads in six steps, with the variance decreasing from 0.04 to 0.0006 (measured in the object frame). The parameter space for each model is divided into cells (""binned""), and then for each image feature a vote is added to each parameter space bin that could have produced that feature. After collecting votes from all image features we then search for peaks in the parameter space accumulator array, and attempt to verify pose. The Hough transform can be viewed as a crude way of approximating the logarithm of the posterior distribution P(zII, M) (e.g. Hunt et al , 1988). However, these two techniques have only been used on problems involving rigid models, and are not readily applicable to the digit recognition problem. For the Hough space method, binning and vote collection is impractical in the high dimensional parameter space, and for the correspondence based approach there is a lack of easily identified and highly discriminative features. On a validation set the squared error of the prediction net was over three times smaller than the trivial net. Although this is encouraging, the acid test is to compare the performance of elastic models settled from the predicted positions using a shortened annealing schedule; if the predictions are good, then only a short amount of settling will be required. The general trends are that the performance obtained using the prediction net is consistently better than the trivial net, and that longer annealing schedules lead to better performance. A comparison of schedules 3 and 4 in table 1 indicates that the performance of the prediction net/schedule 3 combination is similar to (or slightly better than) that obtained with the full annealing schedule, and is more than a factor of two faster. With sufficient matches it is then possible to compute the instantiation parameters of the model. We believe that using machine learning techniques like neural networks to help reduce the amount of search required to fit complex models to data may be useful for many other problems."
1003,Plasticity-Mediated Competitive Learning,"Introduction Throughout the last decades, the game of chess has been a major testbed for research on artificial intelligence and computer science. Most oftoday's chess programs rely on intensive search to generate moves. To evaluate boards, fast evaluation functions are employed which are usually carefully designed by hand, sometimes augmented by automatic parameter tuning methods [1]. Building a chess machine that learns to play solely from the final outcome of games (win/loss/draw) is a challenging open problem in AI. In this paper, we are interested in learning to play chess from the final outcome of games. One of the earliest approaches, which learned solely by playing itself, is Samuel's famous checker player program [10]. His approach employed temporal difference learning (in short: TO) [14], which is a technique for recursively learning an evaluation function . Recently, Tesauro reported the successful application of TO to the game of Backgammon, using artificial neural network representations [16]. This paper presents NeuroChess, a program that learns to play chess from the final outcome of games. The central learning mechanisms is the explanation-based neural network (EBNN) algorithm [9, 8]. Like Tesauro's TD-Gammon approach, NeuroChess constructs a neural network evaluation function for chess boards using TO. In addition, a neural network version of explanation-based learning is employed, which analyzes games in terms of a previously learned neural network chess model. This paper describes the NeuroChess approach, discusses several training issues in the domain of chess, and presents results which elucidate some of its strengths and weaknesses. 2 Temporal Difference Learning in the Domain of Chess Temporal difference learning (TO) [14] comprises a family of approaches to prediction in cases where the event to be predicted may be delayed by an unknown number of time steps. In the context of game playing, TD methods have frequently been applied to learn functions which predict the final outcome of games. Such functions are used as board evaluation functions. In a domain as complex as chess, such an approach might require unreasonably large amounts of training data. Explanation-based methods (EBL) [5, 7, 15] generalize more accurately from less training data. They rely instead on the availability of domain knowledge, which they use for explaining and generalizing training examples. For example, in the explanation of a knight fork, EBL methods employ knowledge about the game of chess to figure out that the position of the queen is relevant, whereas the number of weak pawns is not. Most current approaches to EBL require that the domain knowledge be represented by a set of symbolic rules. Since NeuroChess relies on neural network representations, it employs a neural network version of EBL, called explanation-based neural network learning (EBNN) [9].The evaluation network is trained by Back-propagation and the TD(O) procedure. Both networks are employed for analyzing training example in order to derive target slopes for V. network and thus its first derivative might be erroneous. For the relation ofEBNN and EBL and the accommodation of inaccurate slopes in EBNN see [8]. 4 Training Issues In this section we will briefly discuss some training issues that are essential for learning good evaluation functions in the domain of chess. This list of points has mainly been produced through practical experience with the NeuroChess and related TD approaches. It illustrates the importance of a careful design of the input representation, the sampling rule and the Learning to Play the Game of Chess 1073 parameter setting in a domain as complex as chess. Sampling. The vast majority of chess boards are, loosely speaking, not interesting. If, for example, the opponent leads by more than a queen and a rook, one is most likely to loose. Without an appropriate sampling method there is the danger that the learner spends most of its time learning from uninteresting examples.Training is performed completely asynchronously on up to 20 workstations simultaneously. One of the workstations acts as a weight server, keeping track of the most recent weights and biases of the evaluation network. The other workstations can dynamically establish links to the weight server and contribute to the process of weight refinement. The main process also monitors the state of all other workstations and restarts processes when necessary. The latter component analyzes games using knowledge that was previously learned from expert play. Particular care has been taken in the design of an appropriate feature representation, sampling methods, and parameter settings. Thus far, NeuroChess has successfully managed to beat GNU-Chess in several hundreds of games. However, the level of play still compares poorly to GNU-Chess and human chess players. Despite the initial success, NeuroChess faces two fundamental problems which both might weB be in the way of excellent chess play. Firstly, training time is limited, and it is to be expected that excellent chess skills develop only with excessive training time. This is particularly the case if only the final outcomes are considered. Secondly, with each step of TO-learning NeuroChess loses information."
1004,ICEG Morphology Classification using an Analogue VLSI Neural Network,"The robustness of the neural network architecture reduces the impact of noise, drift and offsets inherent in analogue approaches. The network is a 10:6:3 multi-layer percept ron with on chip digital weight storage, a bucket brigade input to feed the Intracardiac Electrogram (ICEG) to the network and has a winner take all circuit at the output. The network was trained in loop and included a commercial ICD in the signal processing path. The system has successfully distinguished arrhythmia for different patients with better than 90% true positive and true negative detections for dangerous rhythms which cannot be detected by present ICDs. In this paper, we demonstrate that an analogue VLSI neural network can detect such morphology changes while still meeting the strict power and area requirements of an implantable system. Further, since the network is trained in loop with the ICD in real time, the effects of device offsets, noise, QRS detection jitter and signal distortion in the analogue circuits are largely alleviated. The next section discusses the chip circuit designs. Section 5 summarises the results, remaining problems and future directions for the work . 2 ARCHITECTURE The neural network chip consists of a 10:6:3 multilayer perceptron, an input bucket brigade device (BBD) and a winner take all (WTA) circuit at the output. A floor plan and photomicrograph of the chip appears in figure 2. The BBD samples the incoming ICEG at a rate of 250Hz. For three class problems, the winner take all circuit converts the winning class to a digital signal. For the two class problem considered in this paper , a simple thresholding function suffices. The following subsections briefly describe the functional elements of the chip . The circuit diagrams for the chip building blocks appear in figure 3. However, for larger networks with fan-in much greater than ten, an additional problem of common mode cancellation is encountered, That is, as the fan-in increases, a larger common mode range is required or a cancellation scheme using common mode feedback is needed. The neuron of figure 3 implements such a cancellation scheme, The mirrors MO/M2 and Ml/M3 divide the input current and facilitate the sum at the drain of M7. M7/M8 mirrors the sum so that it may be split into two equal currents by the mirrors formed by M4, M5 and M6 which are then subtracted from the input currents. Thus, the differential voltage vp - Vm is a function of the transistor transconductances, the common mode input current and the feedback factor , The gain of the neuron can be controlled by varying the width to length ratio of the mirror transistors MO and Ml. For the case of a neuron which does not cancel the common mode, the power supply voltage must be increased to accommodate the common mode signal, thus leading to a quadratic increase in energy use with fan-in. A common mode cancelling neuron on the other hand , suffers only a linear increase in energy use with fan-in since extra voltage range is not required and the increased energy use arises only due to the linear increase in common mode current. 3 TRAINING SYSTEM The system used to train and test the neural network is shown in figure 4. Control of training and testing takes place on the PC. The PC uses a PC-LAB card to provide analogue and digital I/O . The PC plays the ICEG signal to the input of the commercial ICD in real time. Note, that the PC is only required for initially training the network and in this case as a source of the heart signal. The commercial ICD performs the function of QRS complex detection using analogue circuits. The PC writes weights to the chip via the digital I/Os of the PC-LAB card and the serial weight bus of network. The software package implementing the training and testing, called MUME [Jabri et. al ., 1992], provides a suite of training algorithms and control options. Generalisation Performance of the system on seven patients. runs, whereas, a timing based classifier can not separate these arrhythmia at all. For each convergent weight set the network classified the test set five times. This may be due to the non-linearity of the neuron circuit. Further experiments are required to optimise the architecture for a given patient and to clarify the effect of varying targets, margins and neuron gain. Penalty terms could also be added to the error function to minimise the possibility of missed detections of the dangerous rhythm. The relatively slow rate of the heart results in the best power consumption being obtained by duty cycling the bias currents to the synapses and the buffers."
1005,Real-Time Control of a Tokamak Plasma Using Neural Networks,"INTRODUCTION Fusion of the nuclei of hydrogen provides the energy source which powers the sun. It also offers the possibility of a practically limitless terrestrial source of energy. However, the harnessing of this power has proved to be a highly challenging problem. Early tokamaks had plasmas with circular cross-sections, for which feedback control of the plasma position and shape is relatively straightforward. However, recent tokamaks, such as the COMPASS experiment at Culham Laboratory, as well as most next-generation tokamaks, are designed to produce plasmas whose cross-sections are strongly noncircular. Figure 2 illustrates some of the plasma shapes which COMPASS is designed to explore. These novel cross-sections provide substantially improved energy confinement properties and thereby significantly enhance the performance of the tokamak. z R Figure 1: Schematic cross-section of a tokamak experiment showing the toroidal vacuum vessel (outer D-shaped curve) and plasma (shown shaded). Also shown are the radial (R) and vertical (Z) coordinates. Particular plasma configurations are described in terms of solutions of a non-linear partial differential equation called the Grad-Shafranov (GS) equation. Due to the non-linear nature of this equation, a general analytic solution is not possible. However, the GS equation can be solved by iterative numerical methods, with boundary conditions determined by currents flowing in the external control coils which surround the vacuum vessel. On the tokamak itself it is changes in these currents which are used to alter the position and cross-sectional shape of the plasma. Numerical solution of the GS equation represents the standard technique for post-shot analysis of the plasma, and is also the method used to generate the training dataset for the neural network, as described in the next section. However , this approach is computationally very intensive and is therefore unsuitable for feedback control purposes. Windsor radial distance of the plasma center from the major axis of the torus, vertical distance of the plasma center from the torus midplane, minor radius measured in the plane Z = Zo, elongation, triangularity. We denote these parameters collectively by Yk. The basic problem which has to be addressed, therefore, is to find a representation for the (non-linear) mapping from the magnetic signals m to the values of the geometrical parameters Yk, which can be implemented in suitable hardware for real-time control. This follows from the fact that, if all of the currents are scaled by a constant factor, then the magnetic fields will be scaled by this factor, and the geometry of the plasma boundary will be unchanged . It is important to take advantage of this prior knowledge and to build it into the network structure, rather than force the network to learn it by example. Results from the neural network mapping are compared with those from the optimal linear mapping, that is the single linear transformation which minimizes the same sum-of-squares error as is used in the neural network training algorithm, as this represents the method currently used on a number of present day tokamaks . Initial results were obtained on networks having 3 output units, corresponding to the values of vertical position ZQ, major radius RQ, and elongation K; these being parameters which are of interest for real-time feedback control. The smallest normalized test set error of 11.7 is obtained from the network having 16 hidden units. By comparison, the optimal linear mapping gave a normalized test set error of 18.3. This represents a reduction in error of about 30% in going from the linear mapping to the neural network. Such an improvement, in the context of this application , is very significant. For the experiments on real-time feedback control described in Section 4 the currently available hardware only permitted networks having 4 hidden units, and so we consider the results from this network in more detail. The principal drawback of such an approach is the strong temperature sensitivity due to the appearance of temperature in the denominator of the exponential transistor transfer characteristic. An elegant solution to this problem has been found by exploiting a chip containing 5 transistors in close thermal contact. Two of the transistors form the long-tailed pair, one of the transistors is used as a heat source, and the remaining two transistors are used to measure temperature. External circuitry provides active thermal feedback control, and stability to changes in ambient temperature over the range O?C to 50?C is found to be well within the acceptable range. The complete network is constructed by mounting the appropriate combination of cards in a VME rack and configuring the network topology using front panel interconnections. The system includes extensive diagnostics, allowing voltages at all key points within the network to be monitored as a function of time via a series of multiplexed output channels."
1006,Pulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories,"1 INTRODUCTION Analogue hardware implementations of neural networks have hitherto been hampered by the lack of a straightforward (local) analogue memory capability. The ideal storage mechanism would be compact, non-volatile, easily reprogrammable, and would not interfere with the normal silicon chip fabrication process. Amorphous silicon has been used for synaptic weight storage [1, 2], but only as either a high-resistance fixed weight medium or a binary memory. In this paper, we demonstrate that novel amorphous silicon memory devices can be incorporated into standard CMOS synapse circuits, to provide an analogue weight storage mechanism which is compact, non-volatile, easily reprogrammable, and simple to implement. 2 a-Si:H MEMORY DEVICES The a-Si:H analogue memory device [3] comprises a lOooA thick layer of amorphous silicon (p+ a-Si:H) sandwiched between Vanadium and Chromium electrodes. The a-Si device takes the form of a two-terminal, programmable resistor. It is an ""add-on"" to a conventional CMOS process, and does not demand that the normal CMOS fabrication cycle be disrupted. The a-Si device sits on top of the completed chip circuitry, making contact with the CMOS arithmetic elements via holes cut in the protective passivation layer, as shown in Figure 1. CMOS Passivation Figure 1: The construction of a-Si:H Devices on a CMOS chip After fabrication a number of electronic procedures must be performed in order to program the device to a given resistance state. Programming, and Pre-Programming Procedures Before the a-Si device is usable, the following steps must be carried out: ? Forming: This is a once-only process, applied to the a-Si device in its ""virgin"" state, where it has a resistance of several MO. A series of 300ns pulses, increasing in amplitude from 5v to 14v, is applied to the device electrodes. This creates a vertical conducting channel or filament whose approximate resistance is 1KO. This filament can then be programmed to a value in the range lKO to 1 MO . The details of the physical mechanisms are not yet fully established, but it is clear that conduction occurs through a narrow (sub-micron) conducting channel. Pulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories 765 ? Write: To decrease the device's resistance, negative ""Write"", pulses are applied. ? Erase: To increase the device's resistance, positive"" Erase"" , pulses are applied. ? Usage: Pulses below O.5v do not change the device resistance. The resistance can therefore be utilised as a weight storage medium using a voltage of less than O.5v without causing reprogramming. Programming pulses, which range between 2v and 5v, are typically 120ns in duration. Programming is therefore much faster than for other EEPROM (floating gate) devices used in the same context, which use a series of 100jls pulses to set the threshold voltage [5]. The following sections describe synapse circuits using the a-Si:H devices. For the circuit to function correctly we must limit the voltage on the activity capacitor to the range [1.5v,3.5v], to ensure that the transistors mirroring Isy_z and Iasi remain in saturation. We have also demonstrated the operation of an interface board which allows two 8x8 ANN chips, operating as a two layer network, to be controlled by a simple PC interface card. This technology is most suitable for small networks in, for example, remote control and other embedded-system applications where cost and power considerations favour a single all-inclusive ANN chip with non-volatile, but programmable weights. Another possible application of this technology is in large networks constructed using Thin Film Technology(TFT). If TFT's were used in place of the CMOS transistors then the area constraint imposed by crystalline silicon would be removed, allowing truly massively parallel networks to be integrated. In summary - the a-Si:H analogue memory devices described in this paper provide a route to an analogue, non-volatile and fast synaptic weight storage medium. At the present time neither the programming nor storage mechanisms are fully understood making it difficult to compare this new device with more established technologies such as the ubiquitous Floating-Gate EEPROM technique. Current research is focused on firstly, improving the yield on the a-Si:H device which is unacceptably low at present, a demerit that we attribute to imperfections in the a-Si fabrication process and secondly, improving understanding of the device physics and hence the programming and storage mechanisms."
1007,Learning to Play the Game of Chess,"This illustrates the hardness of the problem of learning to play chess from the final outcome of games. This paper presents NeuroChess, a program that learns to play chess from the final outcome of games. The central learning mechanisms is the explanation-based neural network (EBNN) algorithm [9, 8]. Like Tesauro's TD-Gammon approach, NeuroChess constructs a neural network evaluation function for chess boards using TO. In addition, a neural network version of explanation-based learning is employed, which analyzes games in terms of a previously learned neural network chess model. This paper describes the NeuroChess approach, discusses several training issues in the domain of chess, and presents results which elucidate some of its strengths and weaknesses. 2 Temporal Difference Learning in the Domain of Chess Temporal difference learning (TO) [14] comprises a family of approaches to prediction in cases where the event to be predicted may be delayed by an unknown number of time steps. In the context of game playing, TD methods have frequently been applied to learn functions which predict the final outcome of games. Such functions are used as board evaluation functions. in which the opponent's knight attacks our queen and king simultaneously. Suppose in order to save our king we have to move it, and hence sacrifice our queen. Explanation-based methods (EBL) [5, 7, 15] generalize more accurately from less training data. They rely instead on the availability of domain knowledge, which they use for explaining and generalizing training examples. They can be interpreted as biasing the network V based on chess-specific domain knowledge, embodied in M . For the relation ofEBNN and EBL and the accommodation of inaccurate slopes in EBNN see [8]. 4 Training Issues In this section we will briefly discuss some training issues that are essential for learning good evaluation functions in the domain of chess. This list of points has mainly been produced through practical experience with the NeuroChess and related TD approaches. It illustrates the importance of a careful design of the input representation, the sampling rule and the Learning to Play the Game of Chess 1073 parameter setting in a domain as complex as chess. Sampling. The vast majority of chess boards are, loosely speaking, not interesting. If, for example, the opponent leads by more than a queen and a rook, one is most likely to loose. Without an appropriate sampling method there is the danger that the learner spends most of its time learning from uninteresting examples. Therefore, NeuroChess interleaves selfplay and expert play for guiding the sampling process. More specifically, after presenting a random number of expert moves generated from a large database of grand-master games, NeuroChess completes the game by playing itself. This sampling mechanism has been found to be of major importance to learn a good evaluation function in a reasonable amount of time. Quiescence. In the domain of chess certain boards are harder to evaluate than others. For example, in the middle of an ongoing material exchange, evaluation functions often fail to produce a good assessment. Thus, most chess programs search selectively. A common criterion for determining the depth of search is called quiescence. This criterion basically detects material threats and deepens the search correspondingly. NeuroChess' search engine does the same. Consequently, the evaluation function V is only trained using quiescent boards. Smoothness. Obviously, using the raw, canonical board description as input representation is a poor choice. This is because small changes on the board can cause a huge difference in value, contrasting the smooth nature of neural network representations. Therefore, NeuroChess maps chess board descriptions into a set of board features . These features were carefully designed by hand. Discounting. Software architecture. Training is performed completely asynchronously on up to 20 workstations simultaneously. One of the workstations acts as a weight server, keeping track of the most recent weights and biases of the evaluation network. The other workstations can dynamically establish links to the weight server and contribute to the process of weight refinement. The main process also monitors the state of all other workstations and restarts processes when necessary. Four moves later black is mate. This game is prototypical. As can be seen from this and various other games, NeuroChess has learned successfully to protect its material, to trade material, and to protect its king. It has not learned, however, to open a game in a coordinated way, and it also frequently fails to play short.endgames even if it has a material advantage (this is due to the short planning horizon). Most importantly, it still plays incredibly poor openings, which are often responsible for a draw or a loss. Poor openings do not surprise, however, as TD propagates values from the end of a game to the beginning. Table I shows a performance comparison of NeuroChess versus GNU-Chess, with and without the explanation-based learning strategy. This table illustrates that NeuroChess wins approximately 13% of all games against GNU-Chess, if both use the same search engine. It 'This is because in the current version NeuroChess still heavily uses expert games for sampling. Whenever a grand-master moves its queen to the center of the board, the queen is usually safe, and there is indeed a positive correlation between having the queen in the center and winning in the database. The latter component analyzes games using knowledge that was previously learned from expert play. Particular care has been taken in the design of an appropriate feature representation, sampling methods, and parameter settings. Thus far, NeuroChess has successfully managed to beat GNU-Chess in several hundreds of games. However, the level of play still compares poorly to GNU-Chess and human chess players. Despite the initial success, NeuroChess faces two fundamental problems which both might weB be in the way of excellent chess play. Firstly, training time is limited, and it is to be expected that excellent chess skills develop only with excessive training time. This is particularly the case if only the final outcomes are considered. Secondly, with each step of TO-learning NeuroChess loses information. This is partially because the features used for describing chess boards are incomplete, i.e., knowledge about the feature values alone does not suffice to determine the actual board exactly. But, more importantly, neural networks have not the discriminative power to assign arbitrary values to all possible feature combinations. It is therefore unclear that a TD-like approach will ever, for example, develop good chess openmgs. Another problem of the present implementation is related to the trade-off between knowledge and search. It has been well recognized that the ul timate cost in chess is determi ned by the ti me it takes to generate a move. Chess programs can generally invest their time in search, or in the evaluation of chess boards (search-knowledge trade-off) [3] . Currently, NeuroChess does a poor job, because it spends most of its time computing board evaluations. Computing a large neural network function takes two orders of magnitude longer than evaluating an optimized linear evaluation function (like that of GNU-Chess). VLSI neural network technology offers a promising perspective to overcome this critical shortcoming of sequential neural network simulations. 1076 Sebastian Thrun Acknowledgment The author gratefully acknowledges the guidance and advise by Hans Berliner, who provided the features for representing chess boards, and without whom the current level of play would be much worse. He also thanks Tom Mitchell for his suggestion on the learning methods, and Horst Aurisch for his help with GNU-Chess and the database."
1008,An experimental comparison of recurrent neural networks,"Introduction In the past few years several recurrent neural network architectures have emerged. In this paper we categorize various discrete-time recurrent neural network architectures, and perform a quantitative comparison of these architectures on two problems: grammatical inference and nonlinear system identification. 2 RNN Architectures We broadly divide these networks into two groups depending on whether or not the states of the network are guaranteed to be observable. A network with observable states has the property that the states of the system can always be determined from observations of the input and output alone. The archetypical model in this class .. Also with UMIACS, University of Maryland, College Park, MD 20742 698 Bill G. Horne, C. Lee Giles Table 1: Terms that are weighted in various single layer network architectures. Ui represents the ith input at the current time step, Zi represents the value of the lh node at the previous time step. Architecture First order High order Bilinear Quadratic bias x x Ui Zi x x x x x x UiUj ZiUj ZiZj x x x x x was proposed by Narendra and Parthasarathy [9]. In their most general model, the output of the network is computed by a multilayer perceptron (MLP) whose inputs are a window of past inputs and outputs, as shown in Figure la. A special case of this network is the Time Delay Neural Network (TDNN), which is simply a tapped delay line (TDL) followed by an MLP [7]. This network is not recurrent since there is no feedback; however, the TDL does provide a simple form of dynamics that gives the network the ability model a limited class of nonlinear dynamic systems. A variation on the TDNN, called the Gamma network, has been proposed in which the TDL is replaced by a set of cascaded filters [2]."
101,Training Multilayer Perceptrons with the Extended Kalman Algorithm,"This algorithm converges slowly for large or complex problems such as speech recognition, where thousands of iterations may be needed for convergence even with small data sets. In this paper, we show that training multilayer perceptrons is an identification problem for a nonlinear dynamic system which can be solved using the Extended Kalman Algorithm. Although computationally complex, the Kalman algorithm usually converges in a few iterations. We describe the algorithm and compare it with back-propagation using twodimensional examples. INTRODUCTION Multilayer perceptrons are one of the most popular artificial neural net structures being used today. In most applications, the ""back propagation"" algorithm [Rllmelhart et ai, 1986] is used to train these networks. Although this algorithm works well for small nets or simple problems, convergence is poor if the problem becomes complex or the number of nodes in the network become large [Waibel et ai, 1987]. In problems sllch as speech recognition, tens of thousands of iterations may be required for convergence even with relatively small elata-sets. Thus there is much interest [Prager anel Fallsiele, 1988; Irie and Miyake, 1988] in other ""training algorithms"" which can compute the parameters faster than back-propagation anel/or can handle much more complex problems. In this paper, we show that training multilayer perceptrons can be viewed as an identification problem for a nonlinear dynamic system. For linear dynamic Copyright 1989. Bell Communications Research. Inc. 134 Singhal and Wu systems with white input and observation noise, the Kalman algorithm [Kalman, 1960] is known to be an optimum algorithm. Extended versions of the Kalman algorithm can be applied to nonlinear dynamic systems by linearizing the system around the current estimate of the parameters."
1010,Interference in Learning Internal Models of Inverse Dynamics in Humans,"Introduction In tasks where we use our hands to interact with a tool, our motor system develops a model of the dynamics of that tool and uses this model to control the coupled dynamics of our arm and the tool (Shadmehr and Mussa-Ivaldi 1994). In physical systems theory, the tool is a mechanical analogue of an admittance, mapping a force as input onto a change in state as output (Hogan 1985). In this framework, the ?Currently at Dept. Biomedical Eng, Johns Hopkins Univ, Baltimore, MD 21205 tCurrently at Dept. Physiology, Northwestern Univ Med Sch (M211), Chicago, IL 60611 1118 Reza Shadmehr, Tom Brashers-Krug, Ferdinando Mussa-Ivaldi Figure 1: The experimental setup. The robot is a very low friction planar mechanism powered by two torque motors that act on the shoulder and elbow joints. Subject grips the end-point of the robot which houses a force transducer and moves the hand to a series of targets displayed on a monitor facing the subject (not shown) . The function of the robot is to produce novel force fields that the subject learns to compensate for during reaching movements. model developed by the motor control system during the learning process needs to approximate an inverse of this mapping . This inverse dynamics map is called an internal model of the tool. We have been interested in understanding the representations that the nervous system uses in learning and storing such internal models. In a previous work we measured the way a learned internal model extrapolated beyond the training data (Shadmehr and Mussa-Ivaldi 1994). The results suggested that the coordinate system of the learned map was in intrinsic (e.g., joint or muscles based) rather than in extrinsic (e.g., hand based) coordinates. Here we present a mathematical technique to estimate the input-output properties of the learned map."
1011,Active Learning with Statistical Models,"ACTIVE LEARNING - BACKGROUND An active learning problem is one where the learner has the ability or need to influence or select its own training data. Many problems of great practical interest allow active learning, and many even require it. We consider the problem of actively learning a mapping X - Y based on a set of training examples {(Xi,Yi)}~l' where Xi E X and Yi E Y. The learner is allowed to iteratively select new inputs x (possibly from a constrained set), observe the resulting output y, and incorporate the new examples (x, y) into its training set. The primary question of active learning is how to choose which x to try next. There are many heuristics for choosing x based on intuition, including choosing places where we don't have data, where we perform poorly [Linden and Weber, 1993], where we have low confidence [Thrun and Moller, 1992], where we expect it 706 David Cohn, Zoubin Ghahramani, Michael I. Jordon to change our model [Cohn et aI, 1990], and where we previously found data that resulted in learning [Schmidhuber and Storck, 1993]. In this paper we consider how one may select x ""optimally"" from a statistical viewpoint. We first review how the statistical approach can be applied to neural networks, as described in MacKay [1992] and Cohn [1994]. We then consider two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While optimal data selection for a neural network is computationally expensive and approximate, we find that optimal data selection for the two statistical models is efficient and accurate. 2 ACTIVE LEARNING - A STATISTICAL APPROACH We denote the learner's output given input x as y(x). The mean squared error of this output can be expressed as the sum of the learner's bias and variance. The variance 0'3 (x) indicates the learner's uncertainty in its estimate at x. 1 Our goal will be to select a new example x such that when the resulting example (x, y) is added to the training set, the integrated variance IV is minimized: IV = J0'3 P (x)dx. (1) Here, P(x) is the (known) distribution over X. In practice, we will compute a Monte Carlo approximation of this integral, evaluating 0'3 at a number of random points drawn according to P(x). Selecting x so as to minimize IV requires computing 0-3, the new variance at x given (x, y). Until we actually commit to an x, we do not know what corresponding y we will see, so the minimization cannot be performed deterministically.2 Many learning architectures, however, provide an estimate of PWlx) based on current data, so we can use this estimate to compute the expectation of 0-3. Selecting x to minimize the expected integrated variance provides a solid statistical basis for choosing new examples."
1012,A Rapid Graph-based Method for Arbitrary Transformation-Invariant Pattern Classification,"INTRODUCTION In recent years, the crucial issue of incorporating invariances into networks for pattern recognition has received increased attention, most especially due to the work of 666 Alessandro Sperduti, David G. Stork Simard and his colleagues. To a regular hierachical backpropagation network Simard et al. [1992] added a Jacobian network, which insured that directional derivatives were also learned. Such derivatives represented directions in feature space corresponding to the invariances of interest, such as rotation, translation, scaling and even line thinning. On small training sets for a function approximation problem, this hybrid network showed performance superior to that of a highly tuned backpropagation network taken alone; however there was negligible improvement on large sets. In order to find a simpler method applicable to real-world problems, Simard, Le Cun & Denker [1993] later used a variation of the nearest neighbor algorithm, one incorporating ""tangent distance"" (T-distance or D T ) as the classification metric - the smallest Euclidean distance between patterns after the optimal transformation. In this way, state-of-the-art accuracy was achieved on an isolated handwritten character task, though at quite high computational complexity, owing to the inefficient search and large number of Euclidean and tangent distances that had to be calculated. Whereas Simard, Hastie & Saeckinger [1994] have recently sought to reduce this complexity by means of pre-clustering stored prototypes, we here take a different approach, one in which a (graph) data structure formed during learning contains information about transformations and geometrical relations among prototypes. Nevertheless, it should be noted that our method can be applied to a reduced (clustered) training set such as they formed, yielding yet faster recognition. Simard [1994] recently introduced a hierarchical structure of successively lower resolution patterns, which speeds search only if a minority of patterns are classified more accurately by using the tangent metric than by other metrics. In contrast, our method shows significant improvement even if the majority or all of the patterns are most accurately classified using the tangent distance. Other methods seeking fast invariant classification include Wilensky and Manukian's scheme [1994]. While quite rapid during recall, it is more properly considered distortion (rather than coherent transformation) invariant. Moreover, some transformations such as line thinning cannot be naturally incorporated into their scheme. Finally, it appears as if their scheme scales poorly (compared to tangent metric methods) as the number of invariances is increased. It seems somewhat futile to try to improve significantly upon the recognition accuracy of the tangent metric approach - for databases such as NIST isolated handwritten characters, Simard et al. [1993] reported accuracies matching that of humans! Nevertheless, there remains much that can be done to increase the computational efficiency during recall. This is the problem we address. 2 TRANSFORMATION INVARIANCE In broad overview, during learning our method constructs a labelled graph data structure in which each node represents a stored prototype (labelled by its category) as given by a training set, linked by arcs representing the T-distance between them. Search through this graph (for classification) takes advantage of the graph structure and an improved search criterion. To understand the underlying computations, we must first consider tangent space. Graph-Based Method for Arbitrary Transformation-Invariant Pattern Classification 667 Figure 1: Geometry of tangent space. Here, a three-dimensional feature space contains the ""current"" prototype, Pc, and the subspace consisting of all patterns obtainable by performing continuous transformations of it (shaded). Two candidate prototypes and a test pattern, T, as well as their projections onto the T-space of Pc are shown. The insert (above) shows the progression of search through the corresponding portion of the recognition graph."
1013,Ocular Dominance and Patterned Lateral Connections in a Self-Organizing Model of the Primary Visual Cortex,"The model suggests new computational roles for lateral connections in the cortex, and suggests that the visual cortex maybe maintained in a continuously adapting equilibrium with the visual input by co adapting lateral and afferent connections. 2 The LISSOM Model of Receptive Fields and Ocular Dominance The LISSOM network is a sheet of interconnected neurons (figure 1). Through afferent connections, each neuron receives input from two ""retinas"". In addition, each neuron has reciprocal excitatory and inhibitory lateral connections with other neurons. Lateral excitatory connections are short-range, connecting only close neighbors. Lateral inhibitory connections run for long distances, and may even implement full connectivity between neurons in the network. Neurons receive afferent connections from broad overlapping patches on the retina called anatomical receptive fields, or RFs. The N x N network is projected on to each retina of R x R receptors, and each neuron is connected to receptors in a square area of side s around the projections. Thus, neurons receive afferents from corresponding regions of each retina. Depending on the location of the projection, the number of afferents to a neuron from each retina could vary from x ~s (at the comers) to s x s (at the center). ts The external and lateral weights are organized through an unsupervised learning process. At each training step, neurons start out with zero activity. The initial response TJij of neuron (i, j) Ocular Dominance and Patterned Lateral Connections Loft _ . . 111 fllgIIl Roll . . Figure 1: The Receptive-Field LISSOM architecture. The afferent and lateral connectionsof a single neuron in the liSSOM network are shown. All connection weights are positive. is based on the scalar product TJij = (T (L eabJJij ,ab + a,b L (1) eCdJJij,Cd) , c,d where eab and ecd are the activations of retinal receptors (a, b) and (c, d) within the receptive fields of the neuron in each retina, JJij,ab and JJij,cd are the corresponding afferent weights, and (T is a piecewise linear approximation of the familiar sigmoid activation function. The response evolves over time through lateral interaction. At each time step, the neuron combines the above afferent activation I:: eJJ with lateral excitation and inhibition: TJij(t) = (T (L eJJ + L ""Ie Eij,kITJkl(t - 1) - L ""Ii k,1 Iij,klTJkl(t - 1)) , (2) k,1 where Eij,kl is the excitatory lateral connection weight on the connection from neuron (k, l) to neuron (i, j), Iij,kl is the inhibitory connection weight, and TJkl (t - 1) is the activity of neuron (k, I) during the previous time step. The constants ""Ie and ""Ii determine the relative strengths of excitatory and inhibitory lateral interactions. The activity pattern starts out diffuse and spread over a substantial part of the map, and converges iteratively into stable focused patches of activity, or activity bubbles."
1014,Associative Decorrelation Dynamics: A Theory of Self-Organization and Optimization in Feedback Networks,"ntroduction The mammalian visual system is very effective in detecting the orientations of lines and most neurons in primary visual cortex selectively respond to oriented lines and form orientation columns [1) . Why is the visual system organized as such? We *Present address: Rockefeller University, B272, 1230 York Avenue, NY, NY 10021-6399. 926 Dawei W Dong believe that the visual system is self-organized, in both long term development and short term adaptation, to ensure the optimal information processing. Linsker applied Hebbian learning to model the development of orientation selectivity and later proposed a principle of maximum information preservation in early visual pathways [2]. The focus of his work has been on the feedforward connections and in his model the feedback connections are isotropic and unchanged during the development of orientation columns; but the actual circuitry of visual cortex involves extensive, columnar specified feedback connections which exist even before functional columns appear in cat striate cortex [3]. Our earlier research emphasized the important role of the feedback connections in the development of the columnar structure in visual cortex. We developed a theoretical framework to help understand the dynamics of Hebbian learning in feedback networks and showed how the columnar structure originates from symmetry breaking in the development of the feedback connections (intracortical, or lateral connections within visual cortex) [4]. Figure 1 illustrates our theoretical predictions. The intracortical connections break symmetry and develop strip-like patterns with a characteristic wave length which is comparable to the developed intracortical inhibitory range and the LGN-cortex afferent range (left). The feedforward (LGN-cortex) connections develop under the influence of the symmetry breaking development of the intracortical connections. The developed feedforward connections for each cell form a receptive field which is orientation selective and nearby cells have similar orientation preference (right) . Their orientations change in about the same period as the strip-like pattern of the intracortical connections. Figure 1: The results of the development of visual cortex with feedback connections. The simulated cortex consists of 48 X 48 neurons, each of which connects to 5 X 5 other cortical neurons (left) and receives inputs from 7 X 7 LGN neurons (right). In this figure, white inclicates positive connections and black inclicates negative connections. One can see that the change of receptive field's orientation (right) is highly correlated with the strip-like pattern of intracortical connections (left). Many aspects of our theoretical predictions agree qualitatively with neurobiological observations in primary visual cortex. Another way to test the idea of optimal Associative Correlation Dynamics 927 information processing or any self-organization theory is through quantitative psychophysical studies. The idea is to look for changes in perception following changes in input environments. The psychophysical experiments on orientation illusions offer some opportunities to test our theory on orientation selectivity. Orientation illusions are the effects that the perceived orientations of lines are affected by the neighboring (in time or space) oriented stimuli, which have been observed in many psychophysical experiments and were attributed to the inhibitory interactions between channels tuned to different orientations."
1015,A Connectionist Technique for Accelerated Textual Input: Letting a Network Do the Typing,"This paper describes a neural network system call AutoTypist that monitors a person's typing and predicts what will be entered next. AutoTypist displays the most likely subsequent word to the typist, who can accept it with a single keystroke, instead of typing it in its entirety. The multi-layer perceptron at the heart of Auto'JYpist adapts its predictions of likely subsequent text to the user's word usage pattern, and to the characteristics of the text currently being typed. Increases in typing speed of 2-3% when typing English prose and 10-20% when typing C code have been demonstrated using the system, suggesting a potential time savings of more than 20 hours per user per year. In addition to increasing typing speed, AutoTypist reduces the number of keystrokes a user must type by a similar amount (2-3% for English, 1020% for computer programs). This keystroke savings has the potential to significantly reduce the frequency and severity of repeated stress injuries caused by typing, which are the most common injury suffered in today's office environment. 1 Introduction People in general, and computer professionals in particular, spend a huge amount of time typing. Most of this typing is done sitting in front of a computer display using a keyboard as the primary input device. There are a number of efforts using artificial neural networks and other techniques to improve the comfort and efficiency of human-computer communication using alternative modalities. Speech recognition [Waibel et al., 1988], handwritten character recognition [LeCun et al., 1989], and even gaze tracking [Baluja & Pomerleau, 1993] have 1040 Dean Pomerleau the potential to facilitate this communication. But these technologies are still in their infancy, and at this point cannot approach the speed and accuracy of even a moderately skilled typist for textual input. Is there some way to improve the efficiency of standard keyboard-based human-computer communication? The answer is yes, there are several ways to make typing more efficient. The first, called the Dvorak keyboard, has been around for over 60 years. The Dvorak keyboard has a different arrangement of keys, in which the most common letters, E, T, S, etc., are on the home row right under the typist's fingers. This improved layout requires the typist's fingers to travel1116th as far, resulting in an average of20% increase in typing speed. Unfortunately, the de facto standard in keyboards is the inefficient QWERTY configuration, and people are reluctant to learn a new layout. This paper describes another approach to improving typing efficiency, which can be used with either the QWERTY or DVORAK keyboards. It takes advantage of the hundreds of thousands of computer cycles between the typist's keystrokes which are typically wasted while the computer idly waits for additional input. By spending those cycles trying to predict what the user will type next, and allowing the typist to accept the prediction with a single keystroke, substantial time and effort can be saved over typing the entire text manUally."
1016,Connectionist Speaker Normalization with Generalized Resource Allocating Networks,"INTRODUCTION Speaker normalization methods are designed to minimize inter-speaker variations, one of the principal error sources in automatic speech recognition. Training a speech recognition system on a particular speaker (speaker-dependent or SD mode) generally gives better performance than using a speaker-independent system, which is 868 Cesare Furlanello. Diego Giuliani. Edmondo Trentin trained to recognize speech from a generic user by averaging over individual differences. On the other hand, performance may be dramatically worse when a SD system ""tailored"" on the acoustic characteristics of a speaker (the reference speaker) is used by another one (the new or target speaker). in Figure 1 outlines the general structure of the experiment with GRAN normalization modules. The architecture is independent from the specific speech recognition system and allows comparisons between different normalization techniques. The GRAN model and a general procedure for data standardization are described in Section 2 and 3. After a discussion of the spectral mapping problem in Section 4, the APASCI corpus used in the experiments and the characteristics of the acoustic data are described in Section 5. The recognition system and the experiment set-up are detailed in Sections 6-8. Results are presented and discussed in Section 9. Connectionist Speaker Normalization with Generalized Resource Allocating Networks DataBase: reference phrase 869 phraseS (Yj } j - I ? ...? ] Dynamic Time Warping Training fx) I(Xi(t), Yj(t}} -' Test i-I ?...? I '-------------------""1 Neural Network supervised training : GRAN normalizati Feature Extraction Speech Signal corresponding to phrase S uttered by a new speaker Output Figure 1: System overview 2 THE GRAN MODEL Feedforward artificial neural networks can be regarded as a convenient realization of general functional superpositions in terms of simpler kernel functions (Barron and Barron, 1988). With one hidden layer we can implement a multivariate superposition f(z) = Ef=o cxjKj(z,wj) where Kj is a function depending on an input vector z and a parameter vector Wj, a general structure which allows to realize flexible models for multivariate regression. We are interested in the schema: y = H K(x) + Ax + b with input vector x E Rd 1 and estimated output vector y E R 2 . K = (Kj) is a n-dimensional vector of local kernels, H is the d2 x n real matrix of kernel coefficients, b E R d 2 is an offset term and A is a d2 x d1 linear term. Implemented kernels are Gaussian, Hardy multiquadrics, inverse of Hardy multiquadrics and Epanenchnikov kernels, also in the NadarayaWatson normalized form (HardIe, 1990). The kernel allocation is based on a recursive procedure: if appropriate novelty conditions are satisfied for the example (x', y/), a new kernel Kn+1 is allocated and the new estimate Yn+l becomes Yn+l (x) = Yn(X) + Kn+1 (llx - x'llw)(y' - Yn(X)) (HardIe, 1990). Global properties and rates of convergence for recursive kernel regression estimates are given in (Krzyzak, 1992). The heuristic mechanism suggested by (Platt, 1991) has been extended to include the optimization of the weighted metrics as requested in the generalized versions of RBF networks of (Poggio and Girosi, 1989). Optimization regards kernel coefficients, locations and bandwidths, the offset term, the coefficient matrix A if considered, and the W matrix defining the weighted metrics in the input space: IIxll~ = xtwtWx. Automatic differentiation is used for efficient on-line gradient-descent procedure w.r. t. different error functions (L2, L1, entropy fit), with different learning rates for each type of parameters. 870 Cesare FurLanello, Diego GiuLiani, Edmondo Trentin Ij;-::=<p X -----------+"" Y TJx TJy -1 TJy x -----------"" Y Figure 2: Commutative diagram for the speaker normalization problem. The spectral mapping <p between original spaces X and Y is estimated by Ij; = TJy 1 . ip . TJx, obtained by composition of the neural GRAN mapping ip between PCA spaces X and Y with the two invertible PCA transformations TJx and TJy. 3 NETWORKS AND PCA TRANSFORMATIONS The normalization module is designed to estimate a spectral mapping between the acou"
1017,A Critical Comparison of Models for Orientation and Ocular Dominance Columns in the Striate Cortex,"More than ten of the most prominent models for the structure and for the activity dependent formation of orientation and ocular dominance columns in the striate cort have been evaluated. We implemented those models on parallel machines, we extensively explored parameter space, and we quantitatively compared model predictions with experimental data which were recorded optically from macaque striate cortex. In our contribution we present a summary of our results to date. Briefly, we find that (i) despite apparent differences, many models are based on similar principles and, consequently, make similar predictions, (ii) certain ""pattern models"" as well as the developmental ""correlation-based learning"" models disagree with the experimental data, and (iii) of the models we have investigated, ""competitive Hebbian"" models and the recent model of Swindale provide the best match with experimental data. 1 Models and Data The models for the formation and structure of orientation and ocular dominance columns which we have investigated are summarized in table 1. Models fall into two categories: ""Pattern models"" whose aim is to achieve a concise description of the observed patterns and ""developmental models"" which are focussed on the pro- 94 E. Erwin, K. Obermayer, K. Schulten Class Pattern Models Type Structural Models Spectral Models Develop. Models Correlation Based Learning Competl bve Hebbian Other Model 1. Icecube 2. Pinwheel 3. Gotz 4. Baxter 5. ROJer 6. Niebur 7. Swindale 8. Linsker 9. Miller 10. ~UM-h 11 . SOM-I 12. EN 13. Tanaka 14. Yuille Reference Hubel and Wiesel 1977 [~I Braitenberg and Braitenberg 1979 161 Gotz 1987 (8) Baxter and Dow 1989 11) ROJer and Schwartz 1990 J20) Niebur and Worgotter 1993 (15) Swindale 1992a (21) Linsker 1986c J12] Miller 1989, 1994 113, 14) Ubennayer, et. al. 1990 P~J Obermayer, et. al. 1992(17) Durbin and Mitchison 1990 (7) Tanaka 1991 [22J Yuille, et. al. 1992 (23) Table 1: Models of visual cortical maps which have been evaluated. cesses underlying their formation. Pattern models come in two varieties, ""structural models"" and ""spectral models"", which describe orientation and ocular dominance maps in real."
1018,Generalization in Reinforcement Learning: Safely Approximating the Value Function,"INTRODUCTION Reinforcement learning-the problem of getting an agent to learn to act from sparse, delayed rewards-has been advanced by techniques based on dynamic programming (DP). These algorithms compute a value function which gives, for each state, the minimum possible long-term cost commencing in that state. For the high-dimensional and continuous state spaces characteristic of real-world control tasks, a discrete representation of the value function is intractable; some form of generalization is required. A natural way to incorporate generalization into DP is to use a function approximator, rather than a lookup table, to represent the value function. This approach, which dates back to uses of Legendre polynomials in DP [Bellman et al., 19631, has recently worked well on several dynamic control problems [Mahadevan and Connell, 1990, Lin, 1993] and succeeded spectacularly on the game of backgammon [Tesauro, 1992, Boyan, 1992]. On the other hand, many sensible implementations have been less successful [Bradtke, 1993, Schraudolph et al., 1994]. Indeed, given the well-established success 370 Justin Boyan, Andrew W. Moore on backgammon, the absence of similarly impressive results appearing for other games is perhaps an indication that using function approximation in reinforcement learning does not always work well. In this paper, we demonstrate that the straightforward substitution of function approximators for lookup tables in DP is not robust and, even in very benign cases, may diverge, resulting in an entirely wrong control policy. We then present Grow-Support, a new algorithm designed to converge robustly. Grow-Support grows a collection of states over which function approximation is stable. One-step backups based on Bellman error are not used; instead, values are assigned by performing ""rollouts"" -explicit simulations with a greedy policy. We discuss potential computational advantages of this method and demonstrate its success on some example problems for which the conventional DP algorithm fails. 2 DISCRETE AND SMOOTH VALUE ITERATION Many popular reinforcement learning algorithms, including Q-Iearning and TD(O), are based on the dynamic programmin~ algorithm known as value iteration [Watkins, 1989, Sutton, 1988, Barto et al., 1989J, which for clarity we will call discrete value iteration. Discrete value iteration takes as input a complete model of the world as a Markov Decision Task, and computes the optimal value function J*: J* (x) = the minimum possible sum of future costs starting from x To assure that J* is well-defined, we assume here that costs are nonnegative and that some absorbing goal state-with all future costs O-is reachable from every state. For simplicity we also assume that state transitions are deterministic. Note that J* and the world model together specify a ""greedy"" policy which is optimal for the domain: optimal action from state x = argmin(CosT(x, a) + J*(NEXT-STATE(X, a))) aEA We now consider extending discrete value iteration to the continuous case: we replace the lookup table over all states with a function approximator trained over a sample of states. The smooth value iteration algorithm is given in the appendix. Convergence is no longer guaranteed; we instead recognize four possible classes of behavior: good convergence "
1019,A Mixture Model System for Medical and Machine Diagnosis,"INTRODUCTION Diagnosis is the process of identifying diseases in patients or disorders in machines by considering history, symptoms and other signs through examination. Diagnosis is a common and important problem that has proven hard to automate and formalize. A procedural description is often hard to attain since experts do not know exactly how they solve a problem. In this paper we use the information about a specific problem that exists in a database 1078 Magnus Stensmo. Terrence J. Sejnowski of cases. The disorders or diseases are determined by variables from observations and the goal is to find the probability distribution over the disorders, conditioned on what has been observed. The diagnosis is strong when one or a few of the possible outcomes are differentiated from the others. More information is needed if it is inconclusive. Initially there are only a few clues and the rest of the variables are unknown. Additional information is obtained by asking questions and doing tests. Since tests may be dangerous, time consuming and expensive, it is generally not possible or desirable to find the answer to every question. Unnecessary tests should be avoided . . There have been many attempts to automate diagnosis. Early work [Ledley & Lusted, 1959] realized that the problem is not always tractable due to the large number of influences that can exist between symptoms and diseases. Expert systems, e.g. the INTERNIST system for internal medicine [Miller et al., 1982], have rule-bases which are very hard and time consuming to build. Inconsistencies may arise when new rules are added to an existing database. There is also a strong domain dependence so knowledge bases can rarely be reused for new applications. Bayesian or probabilistic networks [Pearl, 1988] are a way to model a joint probability distribution by factoring using the chain rule in probability theory. Although the models are very powerful when built, there are presently no general learning methods for their construction. A considerable effort is needed. In the Pathfinder system for lymph node pathology [Heckerman et al., 1992] about 14,000 conditional probabilities had to be assessed by an expert pathologist. It is inevitable that errors will occur when such large numbers of manual assessments are involved. Approaches to diagnosis that are based on domain-independent machine learning alleviate some of the problems with knowledge engineering. For decision trees [Quinlan, 1986], a piece of information can only be used if the appropriate question comes up when traversing the tree. This means that irrelevant questions can not be avoided. Feedforward multilayer perceptrons for diagnosis [Baxt, 1990] can classify very well, but they need full information about a case."
102,An Application of the Principle of Maximum Information Preservation to Linear Systems,"I have previously proposed [Linsker, 1987, 1988] a principle of ""maximum information preservation,"" also called the ""infomax"" principle, that may account for certain aspects of the organization of a layered perceptual network. The principle applies to a layer L of cells (which may be the input layer or an intermediate layer of the network) that provides input to a next layer M. The mapping of the input signal vector L onto an output signal vector M, f:L ~ M, is characterized by a conditional probability density function (""pdf"") p(MI L). The set S of allowed mappings I is specified. The input pdf PL(L) is also given. (In the cases considered here, there is no feedback from M to L.) The infomax principle states that a mapping I should be chosen for which the Shannon information rate [Shannon, 1949] R(j) == f dL PL(L) f dM p(MI L) 10g[P(MI L)/PM(M)] (1) is a maximum (over allIin the set S). Here PM(M) == fdLPL(L)P(MIL) is the pdf of the output signal vector M. R is identical to the average mutual information between Land M. 187 Maximum Infonnation Preservation to Linear Systems To understand better how the info max principle may be applied to biological systems and complex synthetic networks, it is useful to solve the infomax optimization problem explicitly for simpler systems whose properties are nonetheless biologically motivated. This paper therefore deals with the practical computation of infomax solutions for cases in which the mappings! are constrained to be linear. INFOMAX SOLUTIONS FOR A SET OF LINEAR FILTERS We consider the case of linear model ""neurons"" with multivariate Gaussian input and additive Gaussian noise."
1020,A Computational Model of Prefrontal Cortex Function,"The model is implemented in continuous-time , thus providing a natural framework in which to study the temporal dynamics of processing in the task. We show how the model can be used to examine the behavioral consequences of neuromodulation in PFC . Specifically, we use the model to make novel and testable predictions regarding the behavioral performance of schizophrenics, who are hypothesized to suffer from reduced dopaminergic tone in this brain area. 1 Introduction Prefrontal cortex (PFC) is an area of the human brain which is significantly expanded relative to other animals. There is general consensus that the PFC is centrally involved in higher cognitive activities such as planning , problem solving and language. Recently, the PFC has been associated with two specific information processing mechanisms : short-term active memory and inhibition . Active memory is the capacity of the nervous system to maintain information in the form of sustained activation states (e.g. , cell firing) for short periods of time. This can be distinguished from forms of memory that are longer in duration and are instantiated as 142 Todd S. Braver, Jonathan D. Cohen, David Servan-Schreiber modified values of physiological parameters (e.g., synaptic strength). Over the last two decades, there have been a large number of neurophysiological studies focusing on the cellular basis of active memory in prefrontal cortex. These studies have revealed neurons in PFC that fire selectively to specific stimuli and response patterns, and that remain active during a delay between these."
1021,The Gamma MLP for Speech Phoneme Recognition,"Infinite Impulse Response (I1R) filters have a significant advantage over Finite Impulse Response (FIR) filters in signal processing: the length of the impulse response is uncoupled from the number of filter parameters. The length of the impulse response is related to the memory depth of a system, and hence I1R filters allow a greater memory depth than FIR filters of the same order. However, I1R filters are *http://www.neci.nj.nec.com/homepages/lawrence 786 S. LAWRENCE, A. C. TSOI, A. D. BACK not widely used in practical adaptive signal processing. This may be attributed to the fact that a) there could be instability during training and b) the gradient descent training procedures are not guaranteed to locate the global optimum in the possibly non-convex error surface (Shynk, 1989). De Vries and Principe proposed using gamma filters (de Vries and Principe, 1992), a special case of IIR filters, at the input to an otherwise standard MLP. The gamma filter is designed to retain the uncoupling of memory depth to the number of parameters provided by IIR filters, but to have simple stability conditions."
1022,A Multiscale Attentional Framework for Relaxation Neural Networks,"We investigate the optimization of neural networks governed by general objective functions. Practical formulations of such objectives are notoriously difficult to solve; a common problem is the poor local extrema that result by any of the applied methods. In this paper, a novel framework is introduced for the solution oflargescale optimization problems. It assumes little about the objective function and can be applied to general nonlinear, non-convex functions; objectives in thousand of variables are thus efficiently minimized by a combination of techniques - deterministic annealing , multiscale optimization, attention mechanisms and trust region optimization methods. 1 INTRODUCTION Many practical problems in computer vision, pattern recognition , robotics and other areas can be described in terms of constrained optimization . In the past decade, researchers have proposed means of solving such problems with the use of neural networks [Hopfield & Tank, 1985; Koch et ai., 1986], which are thus derived as relaxation dynamics for the objective functions codifying the optimization task. One disturbing aspect of the approach soon became obvious , namely the apparent inability of the methods to scale up to practical problems , the principal reason being the rapid increase in the number of local minima present in the objectives as the dimension of the problem increases. Moreover most objectives, E( v), are highly nonlinear, non-convex functions of v , and simple techniques (e.g. steepest descent) D. I. TSIOUTSIAS, E. MJOLSNESS 634 will , in general , locate the first minimum from the starting point. In this work, we propose a framework for solving large-scale instances of such optimization problems."
1023,Correlated Neuronal Response: Time Scales and Mechanisms,"In a previous study of pairs of MT neurons recorded using a single extracellular electrode, it was found that the spike count during two seconds of visual motion stimulation had an average correlation coefficient of r = 0.12 and that this correlation could significantly limit the usefulness of pooling across increasingly large populations of neurons (Zohary et aI., 1994). However, correlated spike count between two neurons could in principle occur at several time-scales. Correlated drifts Correlated Neuronal Response: Time Scales and Mechanisms 69 in the excitability of the cells, for example due to normal biological changes or electrode induced changes, could cause correlation at a time scale of many minutes. Alternatively, attentional or priming effects from higher areas could change the responsivity of the cells at the time scale of an experimental trial. Or, as suggested here, common input that changes on the order of milliseconds could cause correlation in spike count. The first section determines the time scale at which the neurons are correlated by analyzing the relationship between the peak in the spike train cross-correlograms (CCGs) and the correlation between the spike counts using a construct we call the trial CCG. The second section examines temporal structure that is indicative of correlated suppression of firing, perhaps due to inhibition, which may also contribute to the spike count correlation. 2 THE TIME SCALE OF CORRELATION At the time scale of the single trial, the correlation, r se, of spike counts x and y from two neurons recorded during nominally identical two second stimuli was computed using Pearson's correlation coefficient, rse = E[xy] - ExEy , uxuy (1) where E is expected value and u 2 is variance. If spike counts are converted to z-scores, i.e., zero mean and unity variance, then rse = E[xy], and rse may be interpreted as the zero-lag value of the cross-correlation of the z-scored spike counts. The trial CCGs resulting from this procedure are shown for two pairs of neurons in Fig. l. To distinguish between cases like the two shown in Fig. 1, the correlation was broken into a long-term component, rlt, the average value (computed using a Gaussian window of standard deviation 4 trials) surrounding the zero-lag value, and a shortterm component, rst, the difference between the zero-lag value and rlt."
1024,Onset-based Sound Segmentation,"The onset and offset signals are compressed, then clustered both in time and across frequency channels using a network of integrateand-fire neurons. Onsets and offsets are signalled by spikes, and the timing of these spikes used to segment the sound. 1 Background Traditional speech interpretation techniques based on Fourier transforms, spectrum recoding, and a hidden Markov model or neural network interpretation stage have limitations both in continuous speech and in interpreting speech in the presence of noise, and this has led to interest in front ends modelling biological auditory systems for speech interpretation systems (Ainsworth and Meyer 92; Cosi 93; Cole et al 95). Auditory modelling systems use similar early auditory processing to that used in biological systems. Mammalian auditory processing uses two ears, and the incoming signal is filtered first by the pinna (external ear) and the auditory canal before it causes the tympanic membrane (eardrum) to vibrate. This vibration is then passed on through the bones of the middle ear to the oval window on the cochlea. Inside the cochlea, the pressure wave causes a pattern of vibration to occur on the basilar membrane. This appears to be an active process using both the inner and outer hair cells of the organ of Corti. The movement is detected by the inner hair cells and turned into neural impulses by the neurons of the spiral ganglion. These pass down the auditory nerve, and arrive at various parts of the cochlear nucleus. From there, nerve fibres innervate other areas: the lateral and medial nuclei of the superior olive, L.S.SMITH 730 and the inferior colliculus, for example. (See (Pickles 88)). Virtually all modern sound or speech interpretation systems use some form of bandpass filtering, following the biology as far as the cochlea. Most use Fourier transforms to perform a calculation of the energy in each band over some time period, usually between 25 and 75 ms. This is not what the cochlea does. Auditory modelling front ends differ in the extent and length to which they follow animal early auditory processing, but the term generally implies at least that wideband filters are used, and that high temporal resolution is maintained in the initial stages. This means the use of filtering techniques. rather than Fourier transforms in the bandpass stage. Such filtering systems have been implemented by Patterson and Holdsworth (Patterson and Holdsworth 90; Slaney 93), and placed directly in silicon (Lazzaro and Mead 89; Lazzaro et al 93; Liu et al 93; Fragniere and van Schaik 94). Some auditory models have moved beyond cochlear filtering. The inner hair cell has been modelled by either simple rectification (Smith 94) or has been based on the work of (Meddis 88) for example (Patterson and Holdsworth 90; Cosi 93; Brown 92). Lazzaro has experimented with a silicon version of Licklider's autocorrelation processing (Licklider 51; Lazzaro and Mead 89). Others such as (Wu et al 1989: Blackwood et al1990; Ainsworth and Meyer 92; Brown 92; Berthommier 93; Smith 94) have considered the early brainstem nuclei, and their possible contribution, based on the neurophysiology of the different cell types (Pickles 88; Blackburn and Sachs 1989; Kim et al 90). Auditory model-based systems have yet to find their way into mainstream speech recognition systems (Cosi 93). The work presented here uses auditory modelling up to onset cells in the cochlear nucleus. It adds a temporal neural network to clean up the segmentation produced. This part has been filed as a patent (Smith 95). Though the system has some biological plausibility, the aim is an effective data-driven segmentation technique implement able in silicon. 2 Techniques used Digitized sound was applied to an auditory front end, (Patterson and Holdsworth 90), which bandpassed the sound into channels each with bandwidth 24.7{4.37Fr; + I)Hz, where Fe is the centre frequency (in KHz) of the band (Moore and Glasberg 83). These were rectified, modelling the effect of the inner hair cells. The signals produced bear some resemblance to that in the auditory nerve. The real system has far more channels and each nerve channel carries spike-coded information."
1025,A model of transparent motion and non-transparent motion aftereffects,"The model contains two stages of direction selective units. The first stage contains broadly tuned units, while the second stage contains units that are narrowly tuned. The model accounts for the motion aftereffect through adapting units at the first stage and inhibitory interactions at the second stage. The model explains how two populations of dots moving in slightly different directions are perceived as a single population moving in the direction of the vector sum, and how two populations moving in strongly different directions are perceived as transparent motion. The model also explains why the motion aftereffect in both cases appears as non-transparent motion. 1 INTRODUCTION Transparent motion can be studied using displays which contain two populations of moving dots. The dots within each population have the same direction of motion, but directions can differ between the two populations. When the two directions are very similar, subjects report seeing dots moving in the average direction (Williams & Sekuler, 1984). However, when the difference between the two directions gets large, subjects perceive two overlapping sheets of moving dots. This percept is called transparent motion. The occurrence of transparent motion cannot be explained by direction averaging, since that would result in a single direction of perceived motion. Rather than just being a quirk of the human visual system, transparent motion is an important issue in motion processing."
1026,A Model of Auditory Streaming,"The appropriate segmentation and grouping of incoming sensory signals is important in enabling an organism to interact effectively with its environment (Llinas, 1991). The formation of associations between signals, which are considered to arise from the same external source, allows the organism to recognise significant patterns and relationships within the signals from each source without being confused by accidental coincidences between unrelated signals (Bregman, 1990). The intrinsically temporal nature of sound means that in addition to being able to focus on the signal of interest, perhaps of equal significance, is the ability to predict how that signal is expected to progress; such expectations can then be used to facilitate further processing of the signal. It is important to remember that perception is a creative act (Luria, 1980). The organism creates its interpretation of the world in response to the current stimuli, within the context of its current state of alertness, attention, and previous experience. The creative aspects of perception are exemplified in the auditory system where peripheral processing decomposes acoustic stimuli. Since the frequency spectra of complex sounds generally A Model of Auditory Streaming 53 overlap, this poses a complicated problem for the auditory system : which parts of the signal belong together, and which of the subgroups should be associated with each other from one moment to the next, given the extra complication of possible discontinuities and occlusion of sound signals? The process of streaming effectively acts to to associate those sounds emitted from the same source and may be seen as an accomplishment, rather than the breakdown of some integration mechanism (Bregman, 1990). The cognitive model of streaming, proposed by (Bregman, 1990), is based primarily on Gestalt principles such as common fate, proximity, similarity and good continuation. Streaming is seen as a mUltistage process, in which an initial, preattentive process partitions the sensory input, causing successive sounds to be associated depending on the relationship between pitch proximity and presentation rate."
1027,REMAP: Recursive Estimation and Maximization of A Posteriori Probabilities - Application to Transition-Based Connectionist Speech Recognition,"The ultimate goal in speech recognition is to determine the sequence of words that has been uttered. Classical pattern recognition theory shows that the best possible system (in the sense of minimum probability of error) is the one that chooses the word sequence with the maximum a posteriori probability (conditioned on the *Also affiliated with with Faculte Poly technique de Mons, Mons, Belgium REMAP: Recursive Estimation and Maximization of A Posteriori Probabilities 389 evidence). If word sequence i is represented by the statistical model M i , and the evidence (which, for the application reported here, is acoustical) is represented by a sequence X = {Xl, ... , X n , ... , X N }, then we wish to choose the sequence that corresponds to the largest P(MiIX). In (Bourlard & Morgan 1994), summarizing earlier work (such as (Bourlard & Wellekens 1989)), we showed that it was possible to compute the global a posteriori probability P(MIX) of a discriminant form of Hidden Markov Model (Discriminant HMM), M, given a sequence of acoustic vectors X. In Discriminant HMMs, the global a posteriori probability P(MIX) is computed as follows: if r represents all legal paths (state sequences ql, q2, ... , qN) in Mi, N being the length of the sequence, then P(Mi IX) = L P(Mi, ql, q2, ... , qNIX) r = in which ~n represents the specific state hypothesized at time n, from the set Q {ql, ... , q , qk, ... , qK} of all possible HMM states making up all possible models Mi. We can further decompose this into: P(Mi, ql, q2,???, qNIX) = P(ql, q2,???, qNIX)P(Milql, q2,???, qN, X) Under the assumptions stated in (Bourlard & Morgan 1994) we can compute N P(ql, q2,???, qNIX) = II p(qnlqn-l, xn) n=l The Discriminant HMM is thus described in terms of conditional transition probabilities p(q~lq~-l' xn), in which q~ stands for the specific state ql of Q hypothesized at time n and can be schematically represented as in Figure 1. P(IkIIIkI, x) p(/aell/ael, x) P(/aelllkl, x) P(ltIlltI, x) P(ltll/ael, x) Figure 1: An example Discriminant HMM for the word ""cat"". The variable to a specific acoustic observation Xn at time n. X refers Finally, given a state sequence we assume the following approximation: P(Milql, q2,???, qN, X) : : : : P(Milql, q2,???, qN) We can estimate the right side of this last equation from a phonological model (in the case that a given state sequence can belong to two different models). All the required (local) conditional transition probabilities p(q~lq~-l> xn) can be estimated by the Multi-Layer Perceptron (MLP) shown in Figure 2. Recent work at lesl has provided us with further insight into the discriminant HMM, particularly in light of recent work on transition-based models (Konig & Morgan 1994j Morgan et al. 1994). This new perspective has motivated us to further develop the original Discriminant HMM theory. The new approach uses posterior probabilities at both local and global levels and is more discriminant in nature."
1028,Exponentially many local minima for single neurons,"Consider a single artificial neuron with d inputs. The neuron has d weights w E Rd. The output of the neuron for an input pattern x E Rd is y = ?(x? w), where ? : R -+ R is a transfer function. For a given sequence of training examples ((Xt, Yt))I<t<m, each consisting of a pattern Xt E R d and a desired output Yt E R, the goal of the training phase for neural networks consists of minimizing the error function with respect to the weight vector w E Rd. This function is the sum of the losses between outputs of the neuron and the desired outputs summed over all training examples. In notation, the error function is m E(w) = L L(Yt, ?(Xt . w)) , t=1 where L : R x R -+ [0,00) is the loss function. A common example of a transfer function is the logistic function logistic( z) = I+!-' which has the bounded range (0, 1). In contrast, the identity function id(z) = z has unbounded (y - Y)2. Other range. One of the most common loss functions is the square loss L(y, y) examples are the absolute loss Iy - yl and the entropic1oss yin? + (1 - y) In = ::::l We show that for the square loss and the logistic function the error function of a single neuron for n training examples may have Ln / dJ d local minima. More generally, this holds for any loss and transfer function for which the composition of the loss function with the transfer function (in notation L(y, ?(x . w)) is continuous and has bounded range. This Exponentially Many Local Minima for Single Neurons Figure 1: 317 Error Function with 25 Local Minima (16 Visible), Generated by 10 TwoDimensional Examples. proves that for any transfer function with bounded range exponentially many local minima can occur when the loss function is the square loss. The sequences of examples that we use in our proofs have the property that they are nonrealizable in the sense that there is no weight vector W E R d for which the error function is zero, i.e. the neuron cannot produce the desired output for all examples. We show with some minimal assumptions on the loss and transfer functions that for a single neuron there can be no local minima besides the global minimum if the examples are realizable. If the transfer function is the logistic function then it has often been suggested in the literature to use the entropic loss in artificial neural networks in place of the square loss [BW88, WD88, SLF88, Wat92]. In that case the error function of a single neuron is convex and thus has only one minimum even in the non-realizable case. We generalize this observation by defining a matching loss for any differentiable increasing transfer functions ?: 1 ,p-l(y) L</>(y, f)) = (?(z) - y) dz . </>-l(y) The loss is the area depicted in Figure 2a. If ? is the identity function then L</> is the square loss likewise if ? is the logistic function then L</> is the entropic loss."
1029,A Practical Monte Carlo Implementation of Bayesian Learning,"The application of Bayesian learning to neural networks has been pioneered by MacKay (1992), who uses a Gaussian approximation to the posterior weight distribution. However, the Gaussian approximation is poor because of multiple modes in the posterior. Even locally around a mode the accuracy of the Gaussian approximation is questionable, especially when the model is large compared to the amount of training data. Here I present and test a Monte Carlo method (Neal, 1995) which avoids the Gaussian approximation. The implementation is complicated, but the user is not required to have extensive knowledge about the algorithm. Thus, the implementation represents a practical tool for learning in neural nets. 599 A Practical Monte Carlo Implementation of Bayesian Learning 1.1 THE PREDICTION TASK = The training data consists of n examples in the form of inputs x {x(i)} and corresponding outputs y = {y(i)} where i = 1 ... n. For simplicity we consider only real-valued scalar outputs. The network is parametrised by weights w, and hyperparameters h that control the distributions for weights, playing a role similar to that of conventional weight decay. Weights and hyperparameters are collectively termed 0, and the network function is written as F/I (x), although the function value is only indirectly dependent on the hyperparameters (through the weights). Bayes' rule gives the posterior distribution for the parameters in terms of the likelihood, p(ylx, 0), and prior, p(O): p (Olx ,y ) = p(O)p(ylx, O) p(ylx) To minimize the expected squared error on an unseen test case with input we use the mean prediction x(n+l), (1) 2 MONTE CARLO SAMPLING The following implementation is due to Neal (1995). The network weights are updated using the hybrid Monte Carlo method (Duane et al. 1987). This method combines the Metropolis algorithm with dynamical simulation."
103,An Adaptive Network That Learns Sequences of Transitions,"The transition function specifies the current state, St, as a function of the last state and the current input, Ut, (1) In this paper we do not discuss output functions because they are relatively simple. To further simplify discussion, we restrict ourselves to binary input alphabets, although the neural net we describe here can easily be extended to accomodate more complex alphabets. 654 Winter A common engineering problem is to identify and then simulate the functionality of a system from observations of its behavior. Simulation is straightforward when we can actually observe the internal states of a system, since then the function f can be specified by learning simple associations among internal states and external inputs. In robotic systems, for instance, internal states can often be characterized by such parameters as stepper motor settings, strain gauge values, etc., and so are directly accessible. Artificial neural systems have peen found useful in such simulations because they can associate large, possibly noisy state space and input variables with state and output variables (Tolat and Widrow, 1988; Winter, Ryan and Turner, 1987). Unfortunately, in many interesting cases we must base simulations on a limited set of examples of a system's black box behavior because its internal workings are unobservable. The black box description is not, by itself, much use as a simulation tool since usually it cannot be specified without resorting to infinitely large input-output tables. As an alternative we can try to develop a sequential automaton description of the system by observing regularities in its black box behavior. Artificial neural systems can contribute to the development of physical machines dedicated to system identification because i) frequently state representations must be derived from many noisy input variables, ii) data must usually be processed in continuous time and iii) the explicit dynamics of artificial neural systems can be used as a framework for hardware implementations. In this paper we give a brief overview of a neural net, TIN2, which learns and processes state transitions from observations of correct black box behavior when the set of observations is large enough to characterize the black box as an automaton."
1030,Neuron-MOS Temporal Winner Search Hardware for Fully-Parallel Data Processing,"Search for the largest (or the smallest) among a number of input data, Le., the winner-take-all (WTA) action, is an essential part of intelligent data processing such as data retrieval in associative memories [3], vector quantization circuits [4], Kohonen's self-organizing maps [5] etc. In addition to the maximum or minimum search, data sorting also plays an essential role in a number of signal processing such as median filtering in image processing, evolutionary algorithms in optimizing problems [6] and so forth . Usually such data processing is carried out by software running on general purpose computers, but the computation time increases explo- 686 T. SHIBATA, T. NAKAI, T. MORIMOTO, R. KAIHARA, T. YAMASHITA, T. OHMI sively with the increase in the volume of data. In order to build electronic systems having a real-time-response capability, the direct implementation of fully parallel algorithms on the integrated circuits hardware is critically demanded. A variety of WTA [4, 7, 8) circuits have been implemented so far based on analog current-mode circuit technologies. A number of cells, each composed of a current source, competitively share the total current specified by a global current sink and the winner is identified through the current concentration toward the cell via tacit positive feedback mechanisms. The circuit implementations using MOSFET's operating in the subthreshold regime [4, 7) are ideal for large scale integration due to its ultra low power nature. Although they are inherently slow at circuit levels, the performance at a system level is far superior to digital counterparts owing to the flexible computing algorithms of analog. In order to achieve a high speed operation, MOSFET's biased at strong inversion is also utilized in Ref. [8). However, cost must be traded off for increased power."
1031,Dynamics of Attention as Near Saddle-Node Bifurcation Behavior,"Most studies of attention have focused on the selection process of incoming sensory cues (Posner et al., 1980; Koch et al., 1985; Desimone et al., 1995). Emphasis was placed on the phenomena of causing different percepts for the same sensory stimuli. However, the selection of sensory input itself is not the final goal of attention. We consider attention as a means for goal-directed behavior and survival of the animal. In this view, dynamical properties of attention are crucial. While attention has to be maintained long enough to enable robust response to sensory input, it also has to be shifted quickly to a novel cue that is potentially important. Long-term maintenance and quick transition are critical requirements for attention dynamics. ?currently at Dept. of Cognitive Science and Institute for Neural Computation, U. C. San Diego, La Jolla CA 92093-0515. hnakahar@cogsci.ucsd.edu 39 Dynamics of Attention as Near Saddle-node Bifurcation Behavior We investigate a possible neural mechanism that enables those dynamical characteristics of attention. First, we analyze the dynamics of a network of sigmoidal units with self-connections. We show that both long-term maintenance and quick transition can be achieved when the system parameters are near a ""saddle-node bifurcation"" point . Then, we test if such a dynamical mechanism can actually be helpful for an autonomously behaving agent in simulations of a 'bug-eat-food' task. The result indicates that near saddle-node bifurcation behavior can emerge in the course of evolution for survival in non-stationary environments. 2 NEAR SADDLE-NODE BIFURCATION BEHAVIOR When a pulse-like input is given to a linear system, the rising and falling phases of the response have the same time constants. This means that long-term maintenance and quick transition cannot be simultaneously achieved by linear dynamics. Therefore, it is essential to consider a nonlinear dynamical mechanism to achieve these two demands. 2.1 DYNAMICS OF A SELF-RECURRENT UNIT First, we consider the dynamics of a single sigmoidal unit with the self-connection weight a and the bias b. y(t + 1) F(ay(t) + b) , (1) F(x) 1 1 + exp( -x)' (2) The parameters (a, b) determine the qualitative behavior of the system such as the number of fixed points and their stabilities."
1032,VLSI Model of Primate Visual Smooth Pursuit,"The system performed successful velocity tracking for high contrast scenes. Circuit design and performance of the complete smooth pursuit system is presented. 1 INTRODUCTION The smooth pursuit mechanism of primate visual systems is vital for stabilizing a region of the visual field on the retina. The ability to stabilize the image of the world on the retina has profound architectural and computational consequences on the retina and visual cortex, such as reducing the required size, computational speed and communication hardware and bandwidth of the visual system (Bandera, 1990; Eckert and Buchsbaum, 1993). To obtain similar benefits in active machine vision, primate smooth pursuit can be a powerful model for gaze control. The mechanism for smooth pursuit in primates was initially believed to be composed of a simple negative feedback system which attempts to zero the motion of targets on the fovea, figure I (a) (Robinson, 1965). However, this scheme does not account for many psychophysical properties of smooth 707 VLSI Model of Primate Visual Smooth Pursuit pursuit, which led Wyatt and Pola (1979) to proposed figure l(b), where the eye movement signal is added to the target motion in a positive feed back loop. This mechanism results from their observation that eye motion or apparent target motion increases the magnitude of pursuit motion even when retinal motion is zero or constant. Their scheme also exhibited predictive qualities, as reported by Steinbach (1976). The smooth pursuit model presented in this paper attempts the consolidate the two models into a single system which explains the findings of both approaches. Target Moticn Eye Motion Retinal Motion e~ lee G ee = e t G+l ~; > I G ~ co G r Target Motion Eye Motion e~~ > =0 (b) (a) Figure I: System Diagrams of Primate Smooth Pursuit Mechanism. (a) Negative feedback model by Robinson (1965). (b) Positive feedback model by Wyatt and Pola (1979). The velocity based smooth pursuit implemented here attempts to zero the relative velocity of the retina and target. The measured retinal velocity, is zeroed by using positive feedback to accumulate relative velocity error between the target and the retina, where the accumulated value is the current eye velocity. Hence, this model uses the Robinson approach to match target motion, and the Wyatt and Pola positive feed back loop to achieve matching and to predict the future velocity of the target."
1033,Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks,"The approach is based on the fact that an n-dimensional nonlinear dynamical system can be decomposed into one gradient and (n - 1) Hamiltonian systems. Thus, the model selection stage consists of choosing the gradient and Hamiltonian portions appropriately so that a certain behavior is obtainable. To estimate the parameters, a stably convergent learning rule is presented. This algorithm has been proven to converge to the desired system trajectory for all initial conditions and system inputs. This technique can be used to design neural network models which are guaranteed to solve the trajectory learning problem. 1 Introduction A fundamental problem in mathematical systems theory is the identification of dynamical systems. System identification is a dynamic analogue of the functional approximation problem. A set of input-output pairs {u(t), y(t)} is given over some time interval t E [7i, 1j]. The problem is to find a model which for the given input sequence returns an approximation of the given output sequence. Broadly speaking, solving an identification problem involves two steps. The first is choosing a class of identification models which are capable of emulating the behavior of the actual system. The second is selecting a method to determine which member of this class of models best emulates the actual system. In this paper we present a class of nonlinear models and a learning algorithm for these models which are guaranteed to learn the trajectories of an example system. Algorithms to learn given trajectories of a continuous time system have been proposed in [6], [8], and [7] to name only a few. To our knowledge, no one has ever proven that the error between the learned and desired trajectories vanishes for any of these algorithms."
1034,Is Learning The n-th Thing Any Easier Than Learning The First?,"In this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks. 1 Introduction Supervised learning is concerned with approximating an unknown function based on examples. Virtually all current approaches to supervised learning assume that one is given a set of input-output examples, denoted by X, which characterize an unknown function, denoted by f. The target function f is drawn from a class of functions, F, and the learner is given a space of hypotheses, denoted by H, and an order (preference/prior) with which it considers them during learning. For example, H might be the space of functions represented by an artificial neural network with different weight vectors. While this formulation establishes a rigid framework for research in machine learning, it dismisses important aspects that are essential for human learning. Psychological studies have shown that humans often employ more than just the training data for generalization. They are often able to generalize correctly even from a single training example [2, 10]. One of the key aspects of the learning problem faced by humans, which differs from the vast majority of problems studied in the field of neural network learning, is the fact that humans encounter a whole stream of learning problems over their entire lifetime. When faced with a new thing to learn, humans can usually exploit an enormous amount of training data and I also affiliated with: Institut fur Informatik III, Universitat Bonn, Romerstr. 164, Germany Is Learning the n-th Thing Any Easier Than Learning the First? 641 experiences that stem from other, related learning tasks."
1035,A Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex,"The gain of the VOR (the ratio of eye to head rotation velocity) is typically around -1 when the eyes are focused on a distant target. However, to stabilize images accurately, the VOR gain must vary with context (eye position, eye vergence and head translation). We first describe a kinematic model of the VOR which relies solely on sensory information available from the semicircular canals (head rotation), the otoliths (head translation), and neural correlates of eye position and vergence angle. We then propose a dynamical model and compare it to the eye velocity responses measured in monkeys. The dynamical model reproduces the observed amplitude and time course of the modulation of the VOR and suggests one way to combine the required neural signals within the cerebellum and the brain stem. It also makes predictions for the responses of neurons to multiple inputs (head rotation and translation, eye position, etc.) in the oculomotor system. 1 Introduction The VOR stabilizes images on the retina during rapid head motions: Rotations and translations of the head in three dimensions must be compensated by appropriate rotations of the eye. Because the head's rotation axis is not the same as the eye's rotation axis, the calculations for proper image stabilization of an object must take into account diverse variables such as object distance from each eye, O. J. M. D. COENEN, T. J. SEJNOWSKI 90 gaze direction, and head translation (Viire et al., 1986). The stabilization is achieved by integrating infonnation from different sources: head rotations from the semicircular canals of the inner ear, head translations from the otolith organs, eye positions, viewing distance, as well as other context infonnation, such as posture (head tilts) or activity (walking, running) (Snyder and King, 1992; Shelhamer et al.,1992; Grossman et al., 1989). In this paper we concentrate on the context modulation of the VOR which can be described by the kinematics of the reflex, i.e. eye position, eye vergence and head translation."
1036,Improved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms and Network Averaging,"The first method uses a Bayesian prior on the parameter space. We derive EM (Expectation Maximization) update rules which maximize the a posterior parameter probability. In the second approach we apply ensemble averaging to density estimation. This includes Breiman's ""bagging"" , which recently has been found to produce impressive results for classification networks. 1 Introduction Gaussian mixture models have recently attracted wide attention in the neural network community. Important examples of their application include the training of radial basis function classifiers, learning from patterns with missing features, and active learning. The appeal of Gaussian mixtures is based to a high degree on the applicability of the EM (Expectation Maximization) learning algorithm, which may be implemented as a fast neural network learning rule ([Now91], [Orm93]). Severe problems arise, however, due to singularities and local maxima in the log-likelihood function. Particularly in high-dimensional spaces these problems frequently cause the computed density estimates to possess only relatively limited generalization capabilities in terms of predicting the densities of new data points. As shown in this paper, considerably better generalization can be achieved using regularization. 543 Improved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms We will compare two regularization methods. The first one uses a Bayesian prior on the parameters. By using conjugate priors we can derive EM learning rules for finding the MAP (maximum a posteriori probability) parameter estimate. The second approach consists of averaging the outputs of ensembles of Gaussian mixture density estimators trained on identical or resampled data sets. The latter is a form of ""bagging"" which was introduced by Breiman ([Bre94]) and which has recently been found to produce impressive results for classification networks."
1037,Quadratic-Type Lyapunov Functions for Competitive Neural Networks with Different Time-Scales,"This paper investigates a special class of laterally inhibited neural networks. In particular, we have examined the dynamics of a restricted class of laterally inhibited neural networks from a rigorous analytic standpoint. The network models for retinotopic and somatotopic cortical maps are usually composed of several layers of neurons from sensory receptors to cortical units, with feedforward excitations between the layers and lateral (or recurrent) connection within the layer. Standard techniques include (1) Hebbian rule and its variations for modifying synaptic efficacies, (2) lateral inhibition for establishing topographical organization of the cortex, and (3) adiabatic approximation in decoupling the dynamics of relaxation (which is on the fast time scale) and the dynamics of learning (which is on the slow time scale) of the network . However, in most cases, only computer simulation results were obtained and therefore provided limited mathematical understanding of the self-organizating neural response fields. The networks under study model the dynamics of both the neural activity levels, A. MEYER-BASE 338 the short-term memory (STM), and the dynamics of synaptic modifications, the long-term memory (LTM). The actual network models under consideration may be considered extensions of Grossberg's shunting network [Gr076] or Amari's model for primitive neuronal competition [Ama82]. These earlier networks are considered pools of mutually inhibitory neurons with fixed synaptic connections. Our results extended these earlier studies to systems where the synapses can be modified by external stimuli. The dynamics of competitive systems may be extremely complex, exhibiting convergence to point attractors and periodic attractors. For networks which model only the dynamic of the neural activity levels Cohen and Grossberg [CG83] found a Lyapunov function as a necessary condition for the convergence behavior to point attractors. In this paper we apply the results of the theory of Lyapunov functions for singularly perturbed systems on large-scale neural networks, which have two types of state variables (LTM and STM) describing the slow and the fast dynamics of the system. So we can find a Lyapunov function for the neural system with different time-scales and give a design concept of storing desired pattern as stable equilibrium points"
1038,Stable LInear Approximations to Dynamic Programming for Stochastic Control Problems with Local Transitions,"Neural networks are well established in the domains of pattern recognition and function approximation, where their properties and training algorithms have been well studied. Recently, however, there have been some successful applications of neural networks in a totally different context - that of sequential decision making under uncertainty (stochastic control). Stochastic control problems have been studied extensively in the operations research and control theory literature for a long time, using the methodology of dynamic programming [Bertsekas, 1995]. In dynamic programming, the most important object is the cost-to-go (or value) junction, which evaluates the expected future 1046 B. V. ROY, 1. N. TSITSIKLIS cost to be incurred, as a function of the current state of a system. Such functions can be used to guide control decisions. Dynamic programming provides a variety of methods for computing cost-to- go functions. Unfortunately, dynamic programming is computationally intractable in the context of many stochastic control problems that arise in practice. This is because a cost-to-go value is computed and stored for each state, and due to the curse of dimensionality, the number of states grows exponentially with the number of variables involved. Due to the limited applicability of dynamic programming, practitioners often rely on ad hoc heuristic strategies when dealing with stochastic control problems. Several recent success stories - most notably, the celebrated Backgammon player of Tesauro (1992) - suggest that neural networks can help in overcoming this limitation. In these applications, neural networks are used as compact representations that approximate cost- to-go functions using far fewer parameters than states. This approach offers the possibility of a systematic and practical methodology for addressing complex stochastic control problems. Despite the success of neural networks in dynamic programming, the algorithms used to tune parameters are poorly understood. Even when used to tune the parameters of linear approximators, algorithms employed in practice can be unstable [Boyan and Moore, 1995; Gordon, 1995; Tsitsiklis and Van Roy, 1994]. Some recent research has focused on establishing classes of algorithms and compact representation that guarantee stability and graceful error bounds. Tsitsiklis and Van Roy (1994) prove results involving algorithms that employ feature extraction and interpolative architectures. Gordon (1995) proves similar results concerning a closely related class of compact representations called averagers."
1039,Context-Dependent Classes in a Hybrid Recurrent Network-HMM Speech Recognition System,"The ABBOT hybrid connectionist-HMM system performed competitively with many conventional hidden Markov model (HMM) systems in the 1994 ARPA evaluations of speech recognition systems (Hochberg, Cook, Renals, Robinson & Schechtman 1995). This hybrid framework is attractive because it is compact, having far fewer parameters than conventional HMM systems, whilst also providing the discriminative powers of a connectionist architecture. It is well established that particular phones vary acoustically when they occur in different phonetic contexts. For example a vowel may become nasalized when following a nasal sound. The short-term contextual influence of co-articulation is ?Mike Hochberg is now at Nuance Communications, 333 Ravenswood Avenue, Building 110, Menlo Park, CA 94025, USA. Tel: [+1] 415 6148260. Context-dependent Classes in a Speech Recognition System 751 handled in HMMs by creating a model for all sufficiently differing phonetic contexts with enough acoustic evidence. This modelling of phones in their particular phonetic contexts produces sharper probability density functions . This approach vastly improves HMM recognition accuracy over equivalent context-independent systems (Lee 1989). Although the recurrent neural network (RNN) model acoustic context internally (within the state vector) , it does not model phonetic context. This paper presents an approach to improving the ABBOT system through phonetic context-dependent modelling. In Cohen, Franco, Morgan , Rumelhart & Abrash (1992) separate sets of contextdependent output layers are used to model context effects in different states ofHMM phone models. A set of networks discriminate between phones in 8 different broadclass left and right contexts. Training time is reduced by initialising from a CI multilayer perceptron (MLP) and only changing the hidden-to-output weights during context-dependent training. This system performs well on the DARPA Resource Management Task. The work presented in Zhoa, Schwartz , Sroka & Makhoul (1995) followed along similar work to Cohen et al. (1992) . A context-dependent mixture of experts (ME) system (Jordan & Jacobs 1994) based on the structure of the context-independent ME was built. For each state, the whole training data was divided into 46 parts according to its left or right context. Then, a separate ME model was built for each context. Another approach to phonetic context-dependent modelling with MLPs was proposed by Bourlard & Morgan (1993) . It was based on factoring the conditional probability of a phone-in-context given the data in terms of the phone given the data , and its context given the data and the phone. The approach taken in this paper is a mixture of the above work. However, this work augments a recurrent network (rather than an MLP) and concentrates on building a more compact system, which is more suited to our requirements. As a result, the context training scheme is fast and is implemented on a workstation (rather than a parallel processing machine as is used for training the RNN) . OVERVIEW OF THE ABBOT HYBRID SYSTEM The basic framework of the ABBOT system is similar to the one described in Bourlard & Morgan (1994) except that a recurrent network is used as the acoustic model for the within the HMM frameworl."
104,Digital Realisation of Self-Organising Maps,"Background The overall aim of our work is to develop fast and flexible systems for image recognition, usually for commercial inspection tasks. There is an urgent need for automatic learning systems in such applications, since at present most systems employ heuristic classification techniques. This approach requires an extensive development effort for each new application, which exaggerates implementation costs; and for many tasks, there are no clearly defined features which can be employed for classification. Enquiring of a human expert will often only produce ""good"" and ""bad"" examples of each class and not the underlying strategies which he may employ. Our approach is to model in a quite abstract way the perceptual networks found in the mammalian brain for vision. A back-propagation network could be employed to generalise about the input pattern space, and it would find some useful representations. However, there are many difficulties with this approach, since the network structure assumes nothing about the input space and it can be difficult to bound complicated feature clusters using hyperplanes. The mammalian brain is a layered structure, and so another model may be proposed which involves the application of many two-dimensional feature maps. Each map takes information from the output of the preceding one and performs some type of clustering analysis in order to reduce the dimensionality of the input information. For successful recognition, similar patterns must be topologically close so that Digi tal Realisation of Self-Organising Maps novel patterns are in the same general area of the feature map as the class they are most like. There is therefore a need for both global and local ordering processes within the feature map. The process of global ordering in a topological map is termed, by Kohonen (1984), as self-organisation. It Is important to realize that all feedforward networks perform only one function, namely the labelling of areas in a pattern space. This paper concentrates on a technique for realising large, fast, two-dimensional feature maps using a purely digital implementation. Figure 1. Unbounded Feature Map of Local Edges Self Organisation Global ordering needs to adapt the entire neural map, but local ordering needs only local information. Once the optimum global organisation has been found, then only more localised ordering can improve the topological organisation. This process is the basis of the Kohonen clustering algorithm, where the specified area 729 730 Johnson, Allinson and Moon of adaption decreases with time to give an increasing local ordering. It has been shown that this approach gives optimal ordering at global and local levels (Oja, 1983). It may be considered as a dimensionality reduction algorithm, and can be used as a vector quantiser. Although Kohonen's self-organising feature maps have been successfully applied to speech recognition (Kohonen, 1988; Tattersall et aI., 1988), there has been little Investigation in their application for image recognition. Such feature maps can be used to extract various image primitives, such as textures, localised edges and terminations, at various scales of representations (Johnson and Allinson, 1988). As a simple example, a test image of concentric circles is employed to construct a small feature map of localised edges (Figure 1). The distance measure used is the normalised dot product since in general magnitude information is unimportant. Under these conditions, each neuron output can be considered a similarity measure of the directions between the input pattern and the synaptic weight vector. This map shows that similar edges have been grouped together and that inverses are as far from each other as possible. DIGITAL IMPLEMENTATION Sub-Space Classification Although a conventional serial computer is normally thought of as only performing one operation at a time, there is a task which it can successfully perform involving parallel computation. The action of addressing memory can be thought of as a hi&JhlY parallel process, since it involves the comparison of a word, W, with a set ~ 2 others where N is the number of bits in W. It is, in effect, performing 2 parallel computations - each being a single match. This can be exploited to speed up the simulation of a network by using a conversion between conventional pattern space labelling and binary addressing"
1040,Empirical Entropy Manipulation for Real-World Problems,"No finite sample is sufficient to determine the density, and therefore the entropy, of a signal directly. Some assumption about either the functional form of the density or about its smoothness is necessary. Both amount to a prior over the space of possible density functions. By far the most common approach is to assume that the density has a parametric form. By contrast we derive a differential learning rule called EMMA that optimizes entropy by way of kernel density estimation. Entropy and its derivative can then be calculated by sampling from this density estimate. The resulting parameter update rule is surprisingly simple and efficient. We will show how EMMA can be used to detect and correct corruption in magnetic resonance images (MRI). This application is beyond the scope of existing parametric entropy models. 1 Introduction Information theory is playing an increasing role in unsupervised learning and visual processing. For example, Linsker has used the concept of information maximization to produce theories of development in the visual cortex (Linsker, 1988). Becker and Hinton have used information theory to motivate algorithms for visual processing (Becker and Hinton, 1992). Bell and Sejnowski have used information maximization ? Author to whom correspondence should be addressed. Current address: M.LT., 545 Technology Square, Cambridge, MA 02139. P. VIOLA, N. N. SCHRAUDOLPH, T. J. SEJNOWSKI 852 to solve the ""cocktail party"" or signal separation problem (Bell and Sejnowski, 1995). In order to simplify analysis and implementation, each of these techniques makes specific assumptions about the nature of the signals used, typically that the signals are drawn from some parametric density. In practice, such assumptions are very inflexible. In this paper we will derive a procedure that can effectively estimate and manipulate the entropy of a wide variety of signals using non-parametric densities. Our technique is distinguished by is simplicity, flexibility and efficiency. We will begin with a discussion of principal components analysis (PCA) as an example of a simple parametric entropy manipulation technique. After pointing out some of PCA's limitation, we will then derive a more powerful non-parametric entropy manipulation procedure. Finally, we will show that the same entropy estimation procedure can be used to tackle a difficult visual processing problem. 1.1 Parametric Entropy Estimation Typically parametric entropy estimation is a two step process."